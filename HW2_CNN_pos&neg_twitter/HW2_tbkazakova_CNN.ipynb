{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t5cWf-g_qj7"
      },
      "source": [
        "# ДЗ 2. CNN\n",
        "### Таня Казакова"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOoTQyHE_qkC",
        "outputId": "60ee2cb3-b6f8-434c-ac6a-5c9612ff87c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torchvision "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIPXv8EU_qkE",
        "outputId": "36835439-e595-471f-d7a9-750ab28c9b50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.2-py3-none-any.whl (332 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 18.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 235 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 12.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzU76KFpAPb0",
        "outputId": "a3a0700b-ebbd-4bd6-f7b6-d076ada7c783"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 30.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlT39-d1_qkF",
        "outputId": "75a168a0-a3f7-42e9-b55b-f7e5b7ae5300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymystem3 import Mystem\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import collections\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aoq2MlzY_qkH"
      },
      "outputs": [],
      "source": [
        "def lemmatizing(text):\n",
        "    toks = word_tokenize(text)\n",
        "    words = [w.lower() for w in toks if w.isalpha()]\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg5sKBlX_qkN",
        "outputId": "1d74dc04-8b9e-473a-f112-f5767d3e6c02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это бывшая колаб тетрадка. Если хотите у себя перезапустить, надо поменять ссылки на корпусные файлы."
      ],
      "metadata": {
        "id": "dRpeq4sDrkcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZPq1_-LAgu0",
        "outputId": "35a374ac-e715-4409-832f-fe7aea49e408"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NQWYSzQw_qkI"
      },
      "outputs": [],
      "source": [
        "colnames=['id', 'date', 'name', 'text', 'type', 'rep', 'rtw', 'fav', 'stcount', 'foll', 'frien', 'listcount']\n",
        "posit = pd.read_csv('/content/drive/MyDrive/ВШЭ/Прог-е/python-personal/4 курс/positive.csv', sep=\";\", names=colnames)\n",
        "negat = pd.read_csv('/content/drive/MyDrive/ВШЭ/Прог-е/python-personal/4 курс/negative.csv', sep=\";\", names=colnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2051
        },
        "id": "emcE9H3O_qkJ",
        "outputId": "0294d8c3-e495-4376-bc6d-267afdbc2150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6b4358b0-a4d1-4873-9584-7e5bd1f7d71d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "      <th>rep</th>\n",
              "      <th>rtw</th>\n",
              "      <th>fav</th>\n",
              "      <th>stcount</th>\n",
              "      <th>foll</th>\n",
              "      <th>frien</th>\n",
              "      <th>listcount</th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906692374446080</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>pleease_shut_up</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7569</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906692693221377</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>alinakirpicheva</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11825</td>\n",
              "      <td>59</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>408906695083954177</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>EvgeshaRe</td>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1273</td>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>408906695356973056</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>ikonnikova_21</td>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1549</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>408906761416867842</td>\n",
              "      <td>1386325943</td>\n",
              "      <td>JumpyAlex</td>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>597</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226829</th>\n",
              "      <td>425138243257253888</td>\n",
              "      <td>1390195830</td>\n",
              "      <td>Yanch_96</td>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1138</td>\n",
              "      <td>32</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226830</th>\n",
              "      <td>425138339503943682</td>\n",
              "      <td>1390195853</td>\n",
              "      <td>tkit_on</td>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4822</td>\n",
              "      <td>38</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226831</th>\n",
              "      <td>425138437684215808</td>\n",
              "      <td>1390195876</td>\n",
              "      <td>ckooker1</td>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226832</th>\n",
              "      <td>425138490452344832</td>\n",
              "      <td>1390195889</td>\n",
              "      <td>LisaBeroud</td>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2516</td>\n",
              "      <td>187</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226833</th>\n",
              "      <td>425138595251625984</td>\n",
              "      <td>1390195914</td>\n",
              "      <td>sukapavlov</td>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7778</td>\n",
              "      <td>146</td>\n",
              "      <td>66</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226834 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b4358b0-a4d1-4873-9584-7e5bd1f7d71d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b4358b0-a4d1-4873-9584-7e5bd1f7d71d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b4358b0-a4d1-4873-9584-7e5bd1f7d71d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        id        date             name  ... frien  listcount  sent\n",
              "0       408906692374446080  1386325927  pleease_shut_up  ...    61          0     1\n",
              "1       408906692693221377  1386325927  alinakirpicheva  ...    31          2     1\n",
              "2       408906695083954177  1386325927        EvgeshaRe  ...    27          0     1\n",
              "3       408906695356973056  1386325927    ikonnikova_21  ...    17          0     1\n",
              "4       408906761416867842  1386325943        JumpyAlex  ...    23          1     1\n",
              "...                    ...         ...              ...  ...   ...        ...   ...\n",
              "226829  425138243257253888  1390195830         Yanch_96  ...    46          0     0\n",
              "226830  425138339503943682  1390195853          tkit_on  ...    32          0     0\n",
              "226831  425138437684215808  1390195876         ckooker1  ...    16          0     0\n",
              "226832  425138490452344832  1390195889       LisaBeroud  ...   265          0     0\n",
              "226833  425138595251625984  1390195914       sukapavlov  ...    66          5     0\n",
              "\n",
              "[226834 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: total number of rows (226834) exceeds max_rows (20000). Limiting to first (20000) rows.\n"
          ]
        }
      ],
      "source": [
        "posit['sent'] = 1\n",
        "negat['sent'] = 0\n",
        "twits = pd.concat([posit,negat],axis=0, ignore_index=True)\n",
        "twits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "9pm4zVnC_qkK",
        "outputId": "f6fc296f-df1b-4edd-ce57-e0845585b4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 226834/226834 [12:29<00:00, 302.77it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7c03f0a1-298f-4540-91ea-82a32f843084\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "      <th>rep</th>\n",
              "      <th>rtw</th>\n",
              "      <th>fav</th>\n",
              "      <th>stcount</th>\n",
              "      <th>foll</th>\n",
              "      <th>frien</th>\n",
              "      <th>listcount</th>\n",
              "      <th>sent</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906692374446080</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>pleease_shut_up</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7569</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[хоть, я, и, школотый, но, поверь, у, мы, то, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906692693221377</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>alinakirpicheva</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11825</td>\n",
              "      <td>59</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[да, он, немного, похожий, на, он, но, мой, ма...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>408906695083954177</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>EvgeshaRe</td>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1273</td>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[rt, katiacheh, ну, ты, идиотка, я, испугаться...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>408906695356973056</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>ikonnikova_21</td>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1549</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[rt, кто, то, в, угол, сидеть, и, погибать, от...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>408906761416867842</td>\n",
              "      <td>1386325943</td>\n",
              "      <td>JumpyAlex</td>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>597</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[вот, что, значит, страшилка, d, но, блин, пос...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226829</th>\n",
              "      <td>425138243257253888</td>\n",
              "      <td>1390195830</td>\n",
              "      <td>Yanch_96</td>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1138</td>\n",
              "      <td>32</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[но, не, каждый, хотеть, что, то, исправлять, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226830</th>\n",
              "      <td>425138339503943682</td>\n",
              "      <td>1390195853</td>\n",
              "      <td>tkit_on</td>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4822</td>\n",
              "      <td>38</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[скучать, так, только, taaannyaaa, вправлять, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226831</th>\n",
              "      <td>425138437684215808</td>\n",
              "      <td>1390195876</td>\n",
              "      <td>ckooker1</td>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[вот, и, в, школа, в, говно, это, идти, уже, н...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226832</th>\n",
              "      <td>425138490452344832</td>\n",
              "      <td>1390195889</td>\n",
              "      <td>LisaBeroud</td>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2516</td>\n",
              "      <td>187</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[rt, lisaberoud, тауриэль, не, грусть]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226833</th>\n",
              "      <td>425138595251625984</td>\n",
              "      <td>1390195914</td>\n",
              "      <td>sukapavlov</td>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7778</td>\n",
              "      <td>146</td>\n",
              "      <td>66</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>[такси, везти, я, на, работа, раздумывать, при...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226834 rows × 14 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c03f0a1-298f-4540-91ea-82a32f843084')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c03f0a1-298f-4540-91ea-82a32f843084 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c03f0a1-298f-4540-91ea-82a32f843084');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        id  ...                                              words\n",
              "0       408906692374446080  ...  [хоть, я, и, школотый, но, поверь, у, мы, то, ...\n",
              "1       408906692693221377  ...  [да, он, немного, похожий, на, он, но, мой, ма...\n",
              "2       408906695083954177  ...  [rt, katiacheh, ну, ты, идиотка, я, испугаться...\n",
              "3       408906695356973056  ...  [rt, кто, то, в, угол, сидеть, и, погибать, от...\n",
              "4       408906761416867842  ...  [вот, что, значит, страшилка, d, но, блин, пос...\n",
              "...                    ...  ...                                                ...\n",
              "226829  425138243257253888  ...  [но, не, каждый, хотеть, что, то, исправлять, ...\n",
              "226830  425138339503943682  ...  [скучать, так, только, taaannyaaa, вправлять, ...\n",
              "226831  425138437684215808  ...  [вот, и, в, школа, в, говно, это, идти, уже, н...\n",
              "226832  425138490452344832  ...             [rt, lisaberoud, тауриэль, не, грусть]\n",
              "226833  425138595251625984  ...  [такси, везти, я, на, работа, раздумывать, при...\n",
              "\n",
              "[226834 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "twits['words'] = twits['text'].progress_apply(lambda x: lemmatizing(x))\n",
        "twits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyzvKLDo_qkL",
        "outputId": "a9c05261-8414-435c-ef54-44c35b287e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Всего уникальных символов: 141526\n",
            "Уникальных символов, втретившихся больше 2 раз: 35573\n"
          ]
        }
      ],
      "source": [
        "# Делаем word2id и id2word\n",
        "\n",
        "vocab = Counter()\n",
        "for twit in twits['words']:\n",
        "    for word in twit:\n",
        "        vocab[word] +=1\n",
        "print('Всего уникальных символов:', len(vocab))\n",
        "filtered_vocab = set()\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('Уникальных символов, втретившихся больше 2 раз:', len(filtered_vocab))\n",
        "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCiURGcE_qkN"
      },
      "source": [
        "## Первая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sGHC95im_qkO"
      },
      "outputs": [],
      "source": [
        "class TwitDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['words'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['sent'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = list(self.dataset[index])\n",
        "        ids = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "        ids, y = list(zip(*batch))\n",
        "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        return padded_ids, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "r2g5Y20u_qkO",
        "outputId": "bc2b3beb-3e60-4ac6-92c7-ce38cd82ce0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1d88691b-0598-46e3-9072-f2076f905797\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[хоть, я, и, школотый, но, поверь, у, мы, то, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[да, он, немного, похожий, на, он, но, мой, ма...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[rt, katiacheh, ну, ты, идиотка, я, испугаться...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[rt, кто, то, в, угол, сидеть, и, погибать, от...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[вот, что, значит, страшилка, d, но, блин, пос...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226829</th>\n",
              "      <td>[но, не, каждый, хотеть, что, то, исправлять, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226830</th>\n",
              "      <td>[скучать, так, только, taaannyaaa, вправлять, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226831</th>\n",
              "      <td>[вот, и, в, школа, в, говно, это, идти, уже, н...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226832</th>\n",
              "      <td>[rt, lisaberoud, тауриэль, не, грусть]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226833</th>\n",
              "      <td>[такси, везти, я, на, работа, раздумывать, при...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226834 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d88691b-0598-46e3-9072-f2076f905797')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d88691b-0598-46e3-9072-f2076f905797 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d88691b-0598-46e3-9072-f2076f905797');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    words  sent\n",
              "0       [хоть, я, и, школотый, но, поверь, у, мы, то, ...     1\n",
              "1       [да, он, немного, похожий, на, он, но, мой, ма...     1\n",
              "2       [rt, katiacheh, ну, ты, идиотка, я, испугаться...     1\n",
              "3       [rt, кто, то, в, угол, сидеть, и, погибать, от...     1\n",
              "4       [вот, что, значит, страшилка, d, но, блин, пос...     1\n",
              "...                                                   ...   ...\n",
              "226829  [но, не, каждый, хотеть, что, то, исправлять, ...     0\n",
              "226830  [скучать, так, только, taaannyaaa, вправлять, ...     0\n",
              "226831  [вот, и, в, школа, в, говно, это, идти, уже, н...     0\n",
              "226832             [rt, lisaberoud, тауриэль, не, грусть]     0\n",
              "226833  [такси, везти, я, на, работа, раздумывать, при...     0\n",
              "\n",
              "[226834 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "data = twits[['words', 'sent']]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qr1YCaKW_qkP"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = train_test_split(data, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Xju6ebGW_qkS"
      },
      "outputs": [],
      "source": [
        "train_dataset = TwitDataset(train_data, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn,\n",
        "                            sampler=train_sampler, batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KV09am6__qkU"
      },
      "outputs": [],
      "source": [
        "#train_data['sent'].values\n",
        "batch = next(iter(train_iterator))\n",
        "# batch[0].shape\n",
        "# [id2word[int(i)] for i in batch[0][0]]\n",
        "# batch[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FUEzy-nn_qkU"
      },
      "outputs": [],
      "source": [
        "val_dataset = TwitDataset(val_data, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn,\n",
        "                          sampler=val_sampler, batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-4jX0pz_qkU",
        "outputId": "97bf327a-736b-4ec4-ff0a-8d86520e5320"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "val_batch = next(iter(val_iterator))\n",
        "val_batch[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fEPVoiOL_qkb"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.bigrams2 = nn.Conv1d(in_channels=180, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        \n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        cnn_bigrams = self.bigrams2(concat)\n",
        "        pooling1 = cnn_bigrams.max(2)[0] \n",
        "        logits = self.hidden(pooling1) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хорошо бы ReLU делать, но в условии указано не было."
      ],
      "metadata": {
        "id": "vdk0QmFOg76F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3DB3fRVW_qkb"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/(i+1)}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cQ5lYcgU_qkc"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)  # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/(i+1)}, Val f1: {epoch_metric/(i+1)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Qdvqo1Gf_qkc"
      },
      "outputs": [],
      "source": [
        "model = CNN(len(word2id), 186)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWbY9VeK_qkd",
        "outputId": "36f675fd-9e43-4300-f5a3-6fd4f6168323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.633270822252546\n",
            "Train loss: 0.6111162092004504\n",
            "Train loss: 0.597837766011556\n",
            "Train loss: 0.5880684162889208\n",
            "Train loss: 0.5803372754369464\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5294849344662258, Val f1: 0.7184273600578308\n",
            "Val loss: 0.5307746231555939, Val f1: 0.7152470946311951\n",
            "Val loss: 0.530101268064408, Val f1: 0.7168515920639038\n",
            "Val loss: 0.5295495480298996, Val f1: 0.7177861332893372\n",
            "Val loss: 0.5295645461763654, Val f1: 0.7176051139831543\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.538373225265079, Val f1: 0.7058613896369934\n",
            "Val loss: 0.5405049059126112, Val f1: 0.7014437317848206\n",
            "Val loss: 0.5432412447752776, Val f1: 0.6990364789962769\n",
            "Val loss: 0.5435484151045481, Val f1: 0.6997283697128296\n",
            "Val loss: 0.5415586948394775, Val f1: 0.7021689414978027\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.5280010376657759\n",
            "Train loss: 0.5251829313380377\n",
            "Train loss: 0.5215633389495667\n",
            "Train loss: 0.5175592863133975\n",
            "Train loss: 0.5142067858151027\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.48188633152416777, Val f1: 0.7553831338882446\n",
            "Val loss: 0.48033317838396344, Val f1: 0.7560279965400696\n",
            "Val loss: 0.47968755336034863, Val f1: 0.7566547393798828\n",
            "Val loss: 0.47985386018242154, Val f1: 0.7563928961753845\n",
            "Val loss: 0.4795159878049578, Val f1: 0.7570126056671143\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5046490728855133, Val f1: 0.7369593381881714\n",
            "Val loss: 0.5097314069668452, Val f1: 0.7325581312179565\n",
            "Val loss: 0.5133725780027883, Val f1: 0.7291909456253052\n",
            "Val loss: 0.512940956486596, Val f1: 0.7276405692100525\n",
            "Val loss: 0.5105512195163303, Val f1: 0.7290738821029663\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.4799665629863739\n",
            "Train loss: 0.4771615309374673\n",
            "Train loss: 0.47709377124196006\n",
            "Train loss: 0.47577538575444905\n",
            "Train loss: 0.4746117419855935\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4500799536705017, Val f1: 0.7982004880905151\n",
            "Val loss: 0.4495530409472329, Val f1: 0.7978344559669495\n",
            "Val loss: 0.4485084749403454, Val f1: 0.7992495894432068\n",
            "Val loss: 0.4485705963202885, Val f1: 0.7994694709777832\n",
            "Val loss: 0.4490312477520534, Val f1: 0.799392580986023\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4958012534512414, Val f1: 0.7690811157226562\n",
            "Val loss: 0.5016450898514854, Val f1: 0.7640969157218933\n",
            "Val loss: 0.5047475276169954, Val f1: 0.7611910700798035\n",
            "Val loss: 0.5046549298697047, Val f1: 0.7600987553596497\n",
            "Val loss: 0.5029022713502248, Val f1: 0.7606321573257446\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.4438505743231092\n",
            "Train loss: 0.44157485791615075\n",
            "Train loss: 0.44144540031750995\n",
            "Train loss: 0.44298481196165085\n",
            "Train loss: 0.44267295667103357\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40519085441316877, Val f1: 0.8147944211959839\n",
            "Val loss: 0.408684749688421, Val f1: 0.8131713271141052\n",
            "Val loss: 0.41009084497179304, Val f1: 0.812688946723938\n",
            "Val loss: 0.4104371113436563, Val f1: 0.8125153183937073\n",
            "Val loss: 0.41047324793679374, Val f1: 0.8128802180290222\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4801968236764272, Val f1: 0.7635238170623779\n",
            "Val loss: 0.48739608294434017, Val f1: 0.7564759850502014\n",
            "Val loss: 0.49143971889107313, Val f1: 0.7531103491783142\n",
            "Val loss: 0.4916335418820381, Val f1: 0.7522515654563904\n",
            "Val loss: 0.4895155231157939, Val f1: 0.7525469660758972\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.40842858382633757\n",
            "Train loss: 0.4097528440611703\n",
            "Train loss: 0.41110726453009105\n",
            "Train loss: 0.41335363877671105\n",
            "Train loss: 0.41365486758095876\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38412566781044005, Val f1: 0.8340012431144714\n",
            "Val loss: 0.38416802968297686, Val f1: 0.8346460461616516\n",
            "Val loss: 0.3837347748733702, Val f1: 0.8345656991004944\n",
            "Val loss: 0.38371310851403645, Val f1: 0.8342415690422058\n",
            "Val loss: 0.38399223378726416, Val f1: 0.8346770405769348\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.48143009344736737, Val f1: 0.7719955444335938\n",
            "Val loss: 0.49103431900342304, Val f1: 0.7667931914329529\n",
            "Val loss: 0.4949985455583643, Val f1: 0.7636883854866028\n",
            "Val loss: 0.49586741460694206, Val f1: 0.7629383206367493\n",
            "Val loss: 0.4928643988238441, Val f1: 0.7639127969741821\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.3833161337035043\n",
            "Train loss: 0.38609681980950494\n",
            "Train loss: 0.3860127582436516\n",
            "Train loss: 0.3856330118009022\n",
            "Train loss: 0.3859119929586138\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3489828816481999, Val f1: 0.8504021167755127\n",
            "Val loss: 0.3488916060754231, Val f1: 0.8513320088386536\n",
            "Val loss: 0.350276928288596, Val f1: 0.8497671484947205\n",
            "Val loss: 0.3505708694458008, Val f1: 0.8499723076820374\n",
            "Val loss: 0.3504694328989301, Val f1: 0.8499969244003296\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.48024508025911117, Val f1: 0.770473837852478\n",
            "Val loss: 0.48923850225077736, Val f1: 0.7626108527183533\n",
            "Val loss: 0.49370621310340035, Val f1: 0.7601546049118042\n",
            "Val loss: 0.49375051425562966, Val f1: 0.7598907947540283\n",
            "Val loss: 0.4902842852804396, Val f1: 0.761374831199646\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.3515339093548911\n",
            "Train loss: 0.35402814958776746\n",
            "Train loss: 0.35626526985849655\n",
            "Train loss: 0.357616249365466\n",
            "Train loss: 0.3585183768612998\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3222266563347408, Val f1: 0.8660192489624023\n",
            "Val loss: 0.3247437391962324, Val f1: 0.8637897968292236\n",
            "Val loss: 0.3240653239545368, Val f1: 0.863837480545044\n",
            "Val loss: 0.3233063763805798, Val f1: 0.8647733330726624\n",
            "Val loss: 0.3231845298835209, Val f1: 0.8648809790611267\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4875742528173659, Val f1: 0.7661406993865967\n",
            "Val loss: 0.4964463727341758, Val f1: 0.760681688785553\n",
            "Val loss: 0.5007339903601894, Val f1: 0.7577089667320251\n",
            "Val loss: 0.5014419903357824, Val f1: 0.7572952508926392\n",
            "Val loss: 0.4972027838230133, Val f1: 0.7587486505508423\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.32884120345115664\n",
            "Train loss: 0.3295357214553016\n",
            "Train loss: 0.33069375526337397\n",
            "Train loss: 0.33158120278801234\n",
            "Train loss: 0.3319424177919115\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2981263543878283, Val f1: 0.8795134425163269\n",
            "Val loss: 0.2967368909290859, Val f1: 0.8819955587387085\n",
            "Val loss: 0.29636683208601816, Val f1: 0.8822914361953735\n",
            "Val loss: 0.29602408920015605, Val f1: 0.8823196291923523\n",
            "Val loss: 0.29567726254463195, Val f1: 0.8822987675666809\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4957180486785041, Val f1: 0.7684465646743774\n",
            "Val loss: 0.5078596522410711, Val f1: 0.7601954936981201\n",
            "Val loss: 0.5115488701396518, Val f1: 0.7574390172958374\n",
            "Val loss: 0.5124901218546761, Val f1: 0.7567631006240845\n",
            "Val loss: 0.5084580414825015, Val f1: 0.7582848072052002\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.2969049368585859\n",
            "Train loss: 0.3019444857324873\n",
            "Train loss: 0.3036712811106727\n",
            "Train loss: 0.30468861077513015\n",
            "Train loss: 0.3059424999782017\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2657934959445681, Val f1: 0.8976038694381714\n",
            "Val loss: 0.2667284680264337, Val f1: 0.8978360295295715\n",
            "Val loss: 0.26711246697675617, Val f1: 0.8974915146827698\n",
            "Val loss: 0.26783947071858816, Val f1: 0.8968307375907898\n",
            "Val loss: 0.2689862639563424, Val f1: 0.8962764143943787\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5094222890006171, Val f1: 0.7687432765960693\n",
            "Val loss: 0.5210700200663673, Val f1: 0.7620295882225037\n",
            "Val loss: 0.5249507504480856, Val f1: 0.7609288096427917\n",
            "Val loss: 0.5260900664660666, Val f1: 0.7604430317878723\n",
            "Val loss: 0.521478905942705, Val f1: 0.7619142532348633\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.2684314225401197\n",
            "Train loss: 0.27350776067801885\n",
            "Train loss: 0.27748525071711766\n",
            "Train loss: 0.2785778055233615\n",
            "Train loss: 0.2804382183722087\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.24237115808895657, Val f1: 0.911064088344574\n",
            "Val loss: 0.24246717542409896, Val f1: 0.9105063080787659\n",
            "Val loss: 0.24260306173846835, Val f1: 0.9102833867073059\n",
            "Val loss: 0.2433691980583327, Val f1: 0.910011887550354\n",
            "Val loss: 0.24360257378646305, Val f1: 0.9099999666213989\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5256765054331886, Val f1: 0.766970157623291\n",
            "Val loss: 0.536057765285174, Val f1: 0.7628864645957947\n",
            "Val loss: 0.5403544119110814, Val f1: 0.7617422938346863\n",
            "Val loss: 0.5413563988274999, Val f1: 0.7616704106330872\n",
            "Val loss: 0.5366138484742906, Val f1: 0.7627796530723572\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.24418266543320247\n",
            "Train loss: 0.2486047027366502\n",
            "Train loss: 0.25333037702810196\n",
            "Train loss: 0.2554120344775064\n",
            "Train loss: 0.25647977752344947\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.22417306900024414, Val f1: 0.9136236906051636\n",
            "Val loss: 0.2254037869828088, Val f1: 0.9135035872459412\n",
            "Val loss: 0.224514310558637, Val f1: 0.914186954498291\n",
            "Val loss: 0.22487964811069625, Val f1: 0.9139201045036316\n",
            "Val loss: 0.22510781799043927, Val f1: 0.9137454628944397\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5469190875689188, Val f1: 0.754663348197937\n",
            "Val loss: 0.5614072481791178, Val f1: 0.749157190322876\n",
            "Val loss: 0.5683930317560831, Val f1: 0.7481517791748047\n",
            "Val loss: 0.5697836445437537, Val f1: 0.7474958300590515\n",
            "Val loss: 0.5649465991391076, Val f1: 0.7484597563743591\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.22703760436603002\n",
            "Train loss: 0.2276632066283907\n",
            "Train loss: 0.23022111696856362\n",
            "Train loss: 0.23199022603886468\n",
            "Train loss: 0.23354442843369075\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.19835113201822555, Val f1: 0.9307763576507568\n",
            "Val loss: 0.1972944872719901, Val f1: 0.9308916330337524\n",
            "Val loss: 0.1965184251467387, Val f1: 0.9310174584388733\n",
            "Val loss: 0.19690611426319396, Val f1: 0.930787980556488\n",
            "Val loss: 0.19717754534312656, Val f1: 0.9310925006866455\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5637001064088609, Val f1: 0.7643131017684937\n",
            "Val loss: 0.5776680178112454, Val f1: 0.7601169943809509\n",
            "Val loss: 0.5829671047351979, Val f1: 0.758462131023407\n",
            "Val loss: 0.5842853933572769, Val f1: 0.7578741908073425\n",
            "Val loss: 0.5788181212213305, Val f1: 0.7588342428207397\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.20541682371071407\n",
            "Train loss: 0.20504562982491084\n",
            "Train loss: 0.20696996677489507\n",
            "Train loss: 0.2089040892464774\n",
            "Train loss: 0.2106286781174796\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1782030028956277, Val f1: 0.9414120316505432\n",
            "Val loss: 0.17837534759725843, Val f1: 0.9399958252906799\n",
            "Val loss: 0.17790592383770715, Val f1: 0.9402602910995483\n",
            "Val loss: 0.17705244326165745, Val f1: 0.9408382773399353\n",
            "Val loss: 0.17719054579734803, Val f1: 0.9408674836158752\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5888266762097677, Val f1: 0.7676333785057068\n",
            "Val loss: 0.6030835211277008, Val f1: 0.76216059923172\n",
            "Val loss: 0.6099116029562773, Val f1: 0.7602585554122925\n",
            "Val loss: 0.6122402416335212, Val f1: 0.7589181065559387\n",
            "Val loss: 0.6066894133885702, Val f1: 0.759864330291748\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.17866255470684597\n",
            "Train loss: 0.18446487997259411\n",
            "Train loss: 0.18593082825342813\n",
            "Train loss: 0.18799541060413633\n",
            "Train loss: 0.18984711817332675\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1593973342861448, Val f1: 0.9490657448768616\n",
            "Val loss: 0.1595841156584876, Val f1: 0.949189305305481\n",
            "Val loss: 0.15987867187886012, Val f1: 0.9495141506195068\n",
            "Val loss: 0.16083031626684324, Val f1: 0.9491795301437378\n",
            "Val loss: 0.1607762270314353, Val f1: 0.9489128589630127\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6191835337214999, Val f1: 0.7682334184646606\n",
            "Val loss: 0.6338453822665744, Val f1: 0.7619600892066956\n",
            "Val loss: 0.6416376917450516, Val f1: 0.7603726387023926\n",
            "Val loss: 0.6439266171720293, Val f1: 0.7595951557159424\n",
            "Val loss: 0.6374178104930454, Val f1: 0.7602111101150513\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.16366997829505375\n",
            "Train loss: 0.1642639709370477\n",
            "Train loss: 0.1676974967831657\n",
            "Train loss: 0.1689566232264042\n",
            "Train loss: 0.1703690801348005\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.142144918016025, Val f1: 0.9570843577384949\n",
            "Val loss: 0.14034464465720312, Val f1: 0.9574504494667053\n",
            "Val loss: 0.14146346535001483, Val f1: 0.9567992091178894\n",
            "Val loss: 0.14188528939017228, Val f1: 0.9565314054489136\n",
            "Val loss: 0.1418907532947404, Val f1: 0.9564722180366516\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6489051580429077, Val f1: 0.7658392786979675\n",
            "Val loss: 0.6633982161680857, Val f1: 0.7618656754493713\n",
            "Val loss: 0.6710856181603891, Val f1: 0.7600463628768921\n",
            "Val loss: 0.6723658111360338, Val f1: 0.7594597935676575\n",
            "Val loss: 0.666848709848192, Val f1: 0.760391354560852\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.1463302058832986\n",
            "Train loss: 0.1466604681951659\n",
            "Train loss: 0.14830066079185122\n",
            "Train loss: 0.149899431850229\n",
            "Train loss: 0.15182843233857837\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.12575750138078418, Val f1: 0.9643857479095459\n",
            "Val loss: 0.12377274855971336, Val f1: 0.965114176273346\n",
            "Val loss: 0.12440060873826345, Val f1: 0.9648075103759766\n",
            "Val loss: 0.1245308878166335, Val f1: 0.9642745852470398\n",
            "Val loss: 0.12455695075648171, Val f1: 0.9639534950256348\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6695705917146471, Val f1: 0.7670708894729614\n",
            "Val loss: 0.688366111781862, Val f1: 0.760733425617218\n",
            "Val loss: 0.6965685972460994, Val f1: 0.7587977051734924\n",
            "Val loss: 0.6990884575578902, Val f1: 0.7573216557502747\n",
            "Val loss: 0.6931845360332065, Val f1: 0.7574321627616882\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.12421991229057312\n",
            "Train loss: 0.1287675371127469\n",
            "Train loss: 0.1316292078722091\n",
            "Train loss: 0.13311008789709636\n",
            "Train loss: 0.13532705289976937\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.10912140288523266, Val f1: 0.970767080783844\n",
            "Val loss: 0.10929389766284398, Val f1: 0.9704616069793701\n",
            "Val loss: 0.10881164102327257, Val f1: 0.9704127311706543\n",
            "Val loss: 0.10843492443008082, Val f1: 0.9705575108528137\n",
            "Val loss: 0.10812255437885011, Val f1: 0.9707403182983398\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7035310334629483, Val f1: 0.7592328190803528\n",
            "Val loss: 0.7196515334977044, Val f1: 0.7545986175537109\n",
            "Val loss: 0.7276663051711189, Val f1: 0.7536612153053284\n",
            "Val loss: 0.7295104513565699, Val f1: 0.7539666295051575\n",
            "Val loss: 0.7231484029028151, Val f1: 0.7549183368682861\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.11141972265073231\n",
            "Train loss: 0.11468444796545164\n",
            "Train loss: 0.11632514340536936\n",
            "Train loss: 0.1182045536914042\n",
            "Train loss: 0.11977882334164211\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.09748673332589013, Val f1: 0.9739238023757935\n",
            "Val loss: 0.09784746329699244, Val f1: 0.9743569493293762\n",
            "Val loss: 0.09907605243580682, Val f1: 0.973979651927948\n",
            "Val loss: 0.09927032568625041, Val f1: 0.9738040566444397\n",
            "Val loss: 0.09949620706694466, Val f1: 0.9737040996551514\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7430270976490445, Val f1: 0.7615489959716797\n",
            "Val loss: 0.7577643195788065, Val f1: 0.7572218179702759\n",
            "Val loss: 0.7659310146614358, Val f1: 0.7568144202232361\n",
            "Val loss: 0.7718874331977632, Val f1: 0.7565323710441589\n",
            "Val loss: 0.7656048324373034, Val f1: 0.756411075592041\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.09910879475729806\n",
            "Train loss: 0.10088158994913102\n",
            "Train loss: 0.10285151111228126\n",
            "Train loss: 0.10439184292086533\n",
            "Train loss: 0.10581986801964896\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.08493624084762165, Val f1: 0.9796154499053955\n",
            "Val loss: 0.08522040742848601, Val f1: 0.9790929555892944\n",
            "Val loss: 0.08487138077616692, Val f1: 0.9790481925010681\n",
            "Val loss: 0.08464488427021674, Val f1: 0.9790212512016296\n",
            "Val loss: 0.08416382736393384, Val f1: 0.9792678356170654\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.772491368982527, Val f1: 0.7583110928535461\n",
            "Val loss: 0.7946857280201383, Val f1: 0.7538757920265198\n",
            "Val loss: 0.8044425756843002, Val f1: 0.7520796656608582\n",
            "Val loss: 0.8070486055480109, Val f1: 0.7525335550308228\n",
            "Val loss: 0.8001706083615621, Val f1: 0.7524067759513855\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.08644177381481444\n",
            "Train loss: 0.08730515209691865\n",
            "Train loss: 0.08967614436433428\n",
            "Train loss: 0.09122744489993367\n",
            "Train loss: 0.09295188963413238\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.07199050060340337, Val f1: 0.9833467602729797\n",
            "Val loss: 0.07168925282146249, Val f1: 0.9836735129356384\n",
            "Val loss: 0.07173514625146275, Val f1: 0.9839246273040771\n",
            "Val loss: 0.0719948038192732, Val f1: 0.9838642477989197\n",
            "Val loss: 0.07213208483798163, Val f1: 0.983849823474884\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8053897420565287, Val f1: 0.7547683715820312\n",
            "Val loss: 0.826644049750434, Val f1: 0.7530396580696106\n",
            "Val loss: 0.8348273789441144, Val f1: 0.7511223554611206\n",
            "Val loss: 0.8380759838554594, Val f1: 0.7510582208633423\n",
            "Val loss: 0.8310150000784132, Val f1: 0.7512053847312927\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.07275534868240356\n",
            "Train loss: 0.0754599894796099\n",
            "Train loss: 0.07729202735991705\n",
            "Train loss: 0.07876473180949688\n",
            "Train loss: 0.08078805748905454\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0632026582956314, Val f1: 0.9867495894432068\n",
            "Val loss: 0.06322856795574938, Val f1: 0.9869407415390015\n",
            "Val loss: 0.0634072604633513, Val f1: 0.9867183566093445\n",
            "Val loss: 0.06353824327566794, Val f1: 0.986772894859314\n",
            "Val loss: 0.06352396417941365, Val f1: 0.9868318438529968\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8575275871488783, Val f1: 0.7555633783340454\n",
            "Val loss: 0.8740623328420851, Val f1: 0.754073977470398\n",
            "Val loss: 0.8822639694920292, Val f1: 0.752367377281189\n",
            "Val loss: 0.8855654497941335, Val f1: 0.752057671546936\n",
            "Val loss: 0.8783563839064704, Val f1: 0.7519494891166687\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.06347255749361856\n",
            "Train loss: 0.06605128273367881\n",
            "Train loss: 0.06793059539936838\n",
            "Train loss: 0.06976207311132125\n",
            "Train loss: 0.07125255356941905\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.05371331146785191, Val f1: 0.9896754622459412\n",
            "Val loss: 0.054395527339407376, Val f1: 0.9891126155853271\n",
            "Val loss: 0.053582536819435304, Val f1: 0.9895392060279846\n",
            "Val loss: 0.05366487609488624, Val f1: 0.9896434545516968\n",
            "Val loss: 0.05408376649022102, Val f1: 0.9893044233322144\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8868424163924323, Val f1: 0.7520043849945068\n",
            "Val loss: 0.9059541490342882, Val f1: 0.7492926716804504\n",
            "Val loss: 0.9137459264861213, Val f1: 0.7485564351081848\n",
            "Val loss: 0.9177840335501565, Val f1: 0.7481895089149475\n",
            "Val loss: 0.9099216818809509, Val f1: 0.7483200430870056\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.05612803484712328\n",
            "Train loss: 0.05671662613749504\n",
            "Train loss: 0.05838187702354931\n",
            "Train loss: 0.06005653025848525\n",
            "Train loss: 0.06169721856713295\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04880864801151412, Val f1: 0.9904797077178955\n",
            "Val loss: 0.047896743140050345, Val f1: 0.991072952747345\n",
            "Val loss: 0.04743080185282798, Val f1: 0.9911244511604309\n",
            "Val loss: 0.047405121714941094, Val f1: 0.9913010597229004\n",
            "Val loss: 0.04726345313446862, Val f1: 0.9913710951805115\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9221307039260864, Val f1: 0.7514412999153137\n",
            "Val loss: 0.9514942599667443, Val f1: 0.7480670213699341\n",
            "Val loss: 0.9574473456100181, Val f1: 0.7467487454414368\n",
            "Val loss: 0.9605153749386469, Val f1: 0.7469944953918457\n",
            "Val loss: 0.9541715078883701, Val f1: 0.7465683817863464\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.05012928226164409\n",
            "Train loss: 0.05047311979745116\n",
            "Train loss: 0.051345504075288775\n",
            "Train loss: 0.05230823973459857\n",
            "Train loss: 0.05361857586673328\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.040676068195274896, Val f1: 0.9932248592376709\n",
            "Val loss: 0.04135582787649972, Val f1: 0.9928995370864868\n",
            "Val loss: 0.04188861641145888, Val f1: 0.9927969574928284\n",
            "Val loss: 0.041269337944686416, Val f1: 0.9929680824279785\n",
            "Val loss: 0.04114177148256983, Val f1: 0.9930156469345093\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9700452950265672, Val f1: 0.7564157843589783\n",
            "Val loss: 1.007889495955573, Val f1: 0.7516824007034302\n",
            "Val loss: 1.010178460015191, Val f1: 0.7504097819328308\n",
            "Val loss: 1.0107089148627386, Val f1: 0.7503698468208313\n",
            "Val loss: 1.0033655219607882, Val f1: 0.7508625388145447\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.04184292288763183\n",
            "Train loss: 0.04312157215816634\n",
            "Train loss: 0.044800304692415964\n",
            "Train loss: 0.04563289412430355\n",
            "Train loss: 0.04648213235395295\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.03643455452152661, Val f1: 0.9941279888153076\n",
            "Val loss: 0.03583661514733519, Val f1: 0.9940043091773987\n",
            "Val loss: 0.0357076249484505, Val f1: 0.994094729423523\n",
            "Val loss: 0.03549439195277435, Val f1: 0.9941642880439758\n",
            "Val loss: 0.03595298444586141, Val f1: 0.9939914345741272\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0035899546411302, Val f1: 0.7503905296325684\n",
            "Val loss: 1.0345000591542985, Val f1: 0.7481144070625305\n",
            "Val loss: 1.040418185569622, Val f1: 0.7465042471885681\n",
            "Val loss: 1.042637621363004, Val f1: 0.7464355826377869\n",
            "Val loss: 1.0378993113835653, Val f1: 0.7468833327293396\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(25):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель переобучилась. Можно было и гораздо меньше эпох сделать. Минимальный valscore на тесте 0,48... на 5й эпохе  Теперь попробуем исправить и улучшить. Добавим nn.MaxPool1d, nn.ReLU, nn.Dropout."
      ],
      "metadata": {
        "id": "eZPQ65azOpVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 модель"
      ],
      "metadata": {
        "id": "0TgUSjBuPk8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "22YWxgzb_qkZ"
      },
      "outputs": [],
      "source": [
        "# CNN (почти из семинара)\n",
        "class CNN(nn.Module):\n",
        "   \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded))))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        # batch_size x filter_count2\n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((pooling1, pooling2), 1)\n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(len(word2id), 186)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "3xlp0_0yOol0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(40):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clkhVh0sQ73u",
        "outputId": "1cbaa44a-6b94-404e-914c-dfbbd9e28214"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.684097603389195\n",
            "Train loss: 0.647581091097423\n",
            "Train loss: 0.6288207565035139\n",
            "Train loss: 0.6161215041364942\n",
            "Train loss: 0.6071362257003784\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5686890244483948, Val f1: 0.7270284295082092\n",
            "Val loss: 0.5693522427763258, Val f1: 0.7291648387908936\n",
            "Val loss: 0.5693715243112474, Val f1: 0.7295404672622681\n",
            "Val loss: 0.5689700075558254, Val f1: 0.7299821376800537\n",
            "Val loss: 0.569250237260546, Val f1: 0.729559600353241\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5718412266837226, Val f1: 0.7296108603477478\n",
            "Val loss: 0.5736293296019236, Val f1: 0.7267590761184692\n",
            "Val loss: 0.5755414388797901, Val f1: 0.7253339886665344\n",
            "Val loss: 0.5749556471904119, Val f1: 0.7247162461280823\n",
            "Val loss: 0.57382140689426, Val f1: 0.725925862789154\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.5567501391683306\n",
            "Train loss: 0.5534912850175585\n",
            "Train loss: 0.5514048003015064\n",
            "Train loss: 0.5471885638577597\n",
            "Train loss: 0.5440775282042367\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5297710248402187, Val f1: 0.7532320022583008\n",
            "Val loss: 0.5309149103505271, Val f1: 0.7559752464294434\n",
            "Val loss: 0.5307819275628953, Val f1: 0.7554598450660706\n",
            "Val loss: 0.5305716999939509, Val f1: 0.7565106153488159\n",
            "Val loss: 0.5299126543317523, Val f1: 0.7571456432342529\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5382092661327786, Val f1: 0.7526313066482544\n",
            "Val loss: 0.5413040253851149, Val f1: 0.7476373314857483\n",
            "Val loss: 0.5431908302836947, Val f1: 0.7447916865348816\n",
            "Val loss: 0.5424163258737988, Val f1: 0.7437213063240051\n",
            "Val loss: 0.5410848140716553, Val f1: 0.7437722086906433\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5184753690447126\n",
            "Train loss: 0.5161103299685887\n",
            "Train loss: 0.5159659209705535\n",
            "Train loss: 0.5146056615880558\n",
            "Train loss: 0.5137607175963266\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.504541449035917, Val f1: 0.7767037153244019\n",
            "Val loss: 0.5039305273975645, Val f1: 0.7767082452774048\n",
            "Val loss: 0.5034472105048952, Val f1: 0.7780119180679321\n",
            "Val loss: 0.5034226432442666, Val f1: 0.7771340608596802\n",
            "Val loss: 0.5034632475035531, Val f1: 0.7767201662063599\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5179798007011414, Val f1: 0.7612350583076477\n",
            "Val loss: 0.5219192405541738, Val f1: 0.7566908001899719\n",
            "Val loss: 0.5238239654788265, Val f1: 0.7528530955314636\n",
            "Val loss: 0.5228320095274184, Val f1: 0.7531519532203674\n",
            "Val loss: 0.5213122546672821, Val f1: 0.7535325288772583\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.49098077416419983\n",
            "Train loss: 0.49219649178641184\n",
            "Train loss: 0.4914432071504139\n",
            "Train loss: 0.4914118726338659\n",
            "Train loss: 0.4909852875982012\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4861120658261435, Val f1: 0.7924041152000427\n",
            "Val loss: 0.4864702544042042, Val f1: 0.7931227684020996\n",
            "Val loss: 0.4873138782523927, Val f1: 0.7915614247322083\n",
            "Val loss: 0.48709253349474496, Val f1: 0.7919992208480835\n",
            "Val loss: 0.4868038593019758, Val f1: 0.7920831441879272\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5105096399784088, Val f1: 0.7710127234458923\n",
            "Val loss: 0.5145570387442907, Val f1: 0.7648594975471497\n",
            "Val loss: 0.5161069642614435, Val f1: 0.7628051042556763\n",
            "Val loss: 0.5149444250596894, Val f1: 0.7627343535423279\n",
            "Val loss: 0.5133461217085521, Val f1: 0.7628920674324036\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.47747817890984673\n",
            "Train loss: 0.4742834951196398\n",
            "Train loss: 0.474124531802677\n",
            "Train loss: 0.47394930337156566\n",
            "Train loss: 0.47331141778400965\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4608111943517412, Val f1: 0.7995331883430481\n",
            "Val loss: 0.46337782101971764, Val f1: 0.7989763617515564\n",
            "Val loss: 0.4634416858355204, Val f1: 0.7988976240158081\n",
            "Val loss: 0.4624779543706349, Val f1: 0.8001745343208313\n",
            "Val loss: 0.462521949495588, Val f1: 0.7999848127365112\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.49304522739516365, Val f1: 0.7652736306190491\n",
            "Val loss: 0.49729668762948775, Val f1: 0.7611504793167114\n",
            "Val loss: 0.498908790173354, Val f1: 0.7605553269386292\n",
            "Val loss: 0.497485531700982, Val f1: 0.760283350944519\n",
            "Val loss: 0.4958895206451416, Val f1: 0.761487603187561\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.45843238149370463\n",
            "Train loss: 0.45678864121437074\n",
            "Train loss: 0.4573438851606278\n",
            "Train loss: 0.4571946720991816\n",
            "Train loss: 0.45771871072905407\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4499178562845503, Val f1: 0.8088417053222656\n",
            "Val loss: 0.4487598078591483, Val f1: 0.8098746538162231\n",
            "Val loss: 0.44898569158145357, Val f1: 0.8100468516349792\n",
            "Val loss: 0.4491235036935125, Val f1: 0.8103659749031067\n",
            "Val loss: 0.44890394006456646, Val f1: 0.8111715912818909\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4861574371655782, Val f1: 0.7702496647834778\n",
            "Val loss: 0.49074921839767033, Val f1: 0.7665910720825195\n",
            "Val loss: 0.49261528143176325, Val f1: 0.7649447321891785\n",
            "Val loss: 0.4911586907174852, Val f1: 0.7650797963142395\n",
            "Val loss: 0.4894447320037418, Val f1: 0.765827476978302\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.4427916944026947\n",
            "Train loss: 0.4438028539930071\n",
            "Train loss: 0.44359966771943227\n",
            "Train loss: 0.4455354590501104\n",
            "Train loss: 0.4453467401436397\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.433705016544887, Val f1: 0.8199108242988586\n",
            "Val loss: 0.43473606748240334, Val f1: 0.8198836445808411\n",
            "Val loss: 0.43403568154289607, Val f1: 0.8207351565361023\n",
            "Val loss: 0.4346359570111547, Val f1: 0.820393443107605\n",
            "Val loss: 0.4345796241079058, Val f1: 0.8204613327980042\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.48115068342950607, Val f1: 0.7721598744392395\n",
            "Val loss: 0.48562576870123547, Val f1: 0.7691243290901184\n",
            "Val loss: 0.4870967357246964, Val f1: 0.767240583896637\n",
            "Val loss: 0.4853125827180015, Val f1: 0.767703652381897\n",
            "Val loss: 0.48368274834420943, Val f1: 0.7687997221946716\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.4322908682482583\n",
            "Train loss: 0.43174130235399516\n",
            "Train loss: 0.4336882568541027\n",
            "Train loss: 0.43340969490153447\n",
            "Train loss: 0.4325492794173104\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4257592814309256, Val f1: 0.830170750617981\n",
            "Val loss: 0.42524845600128175, Val f1: 0.8294097185134888\n",
            "Val loss: 0.424474078700656, Val f1: 0.8295596241950989\n",
            "Val loss: 0.424831389103617, Val f1: 0.8295371532440186\n",
            "Val loss: 0.4245333887849535, Val f1: 0.8298008441925049\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4805985920959049, Val f1: 0.7756360173225403\n",
            "Val loss: 0.4850972195466359, Val f1: 0.774596631526947\n",
            "Val loss: 0.4863035601598245, Val f1: 0.7730082273483276\n",
            "Val loss: 0.4844452465573947, Val f1: 0.7734863758087158\n",
            "Val loss: 0.48265366819169786, Val f1: 0.7741426229476929\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.4233792381627219\n",
            "Train loss: 0.41994010508060453\n",
            "Train loss: 0.41938360787573314\n",
            "Train loss: 0.4209586098790169\n",
            "Train loss: 0.4210056451388768\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4110142452376229, Val f1: 0.837020993232727\n",
            "Val loss: 0.41300668333257945, Val f1: 0.8341749310493469\n",
            "Val loss: 0.4110477688766661, Val f1: 0.8362395763397217\n",
            "Val loss: 0.41085652666432515, Val f1: 0.8366928100585938\n",
            "Val loss: 0.4111297643184662, Val f1: 0.8363460898399353\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.47588245073954266, Val f1: 0.7730329036712646\n",
            "Val loss: 0.4801896545622084, Val f1: 0.7736313939094543\n",
            "Val loss: 0.48153768314255607, Val f1: 0.7732132077217102\n",
            "Val loss: 0.4793415889143944, Val f1: 0.7744656205177307\n",
            "Val loss: 0.477493863635593, Val f1: 0.7750526666641235\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.41043168561799187\n",
            "Train loss: 0.4115615589278085\n",
            "Train loss: 0.4108707723163423\n",
            "Train loss: 0.41016409801585335\n",
            "Train loss: 0.41003606063979015\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40122141923223226, Val f1: 0.8448166847229004\n",
            "Val loss: 0.4017179438046047, Val f1: 0.8436458706855774\n",
            "Val loss: 0.4013299448149545, Val f1: 0.8436506390571594\n",
            "Val loss: 0.40125358189855304, Val f1: 0.84348064661026\n",
            "Val loss: 0.4011648290497916, Val f1: 0.8438932299613953\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4740138550599416, Val f1: 0.7725608348846436\n",
            "Val loss: 0.47830988301171196, Val f1: 0.7723367810249329\n",
            "Val loss: 0.4791188769870334, Val f1: 0.7722240686416626\n",
            "Val loss: 0.4770911915434731, Val f1: 0.7735829949378967\n",
            "Val loss: 0.4750463200940026, Val f1: 0.7744181752204895\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.3967727644102914\n",
            "Train loss: 0.3987177027123315\n",
            "Train loss: 0.39969634328569686\n",
            "Train loss: 0.40037779211997987\n",
            "Train loss: 0.40077687348638263\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39247443165097917, Val f1: 0.849159836769104\n",
            "Val loss: 0.3926653291497912, Val f1: 0.8486291766166687\n",
            "Val loss: 0.391033730336598, Val f1: 0.8500618934631348\n",
            "Val loss: 0.39126541848693575, Val f1: 0.8499727845191956\n",
            "Val loss: 0.39116237861769537, Val f1: 0.8496320247650146\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4721427692307366, Val f1: 0.7746211290359497\n",
            "Val loss: 0.4768817010853026, Val f1: 0.7741653919219971\n",
            "Val loss: 0.4779907464981079, Val f1: 0.7742069363594055\n",
            "Val loss: 0.47577574021286434, Val f1: 0.775750458240509\n",
            "Val loss: 0.4736404657363892, Val f1: 0.7763186097145081\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.3890823815550123\n",
            "Train loss: 0.3894277908972332\n",
            "Train loss: 0.39025305906931557\n",
            "Train loss: 0.39081624235425677\n",
            "Train loss: 0.39080786279269625\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38132999965122766, Val f1: 0.8540604710578918\n",
            "Val loss: 0.37843873628548214, Val f1: 0.856023907661438\n",
            "Val loss: 0.37895969379515876, Val f1: 0.855266809463501\n",
            "Val loss: 0.37930641302040646, Val f1: 0.854762613773346\n",
            "Val loss: 0.3796665045193263, Val f1: 0.8545624613761902\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4692991574605306, Val f1: 0.77252197265625\n",
            "Val loss: 0.47392148276170093, Val f1: 0.7723591923713684\n",
            "Val loss: 0.4750451445579529, Val f1: 0.7729308009147644\n",
            "Val loss: 0.47251131799485946, Val f1: 0.7742778062820435\n",
            "Val loss: 0.47019840015305414, Val f1: 0.7751426100730896\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.38221700191497804\n",
            "Train loss: 0.3796133858816964\n",
            "Train loss: 0.37901418521290736\n",
            "Train loss: 0.3795985909444945\n",
            "Train loss: 0.3814188865252904\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36946032047271726, Val f1: 0.8623308539390564\n",
            "Val loss: 0.3693326013428824, Val f1: 0.8629555702209473\n",
            "Val loss: 0.3702648344494048, Val f1: 0.8618848323822021\n",
            "Val loss: 0.37091378761189325, Val f1: 0.8612415194511414\n",
            "Val loss: 0.37103656462260654, Val f1: 0.860728919506073\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4684670435057746, Val f1: 0.7740228176116943\n",
            "Val loss: 0.4731739925013648, Val f1: 0.7724764347076416\n",
            "Val loss: 0.4742599339396865, Val f1: 0.7732155323028564\n",
            "Val loss: 0.4718355991774135, Val f1: 0.7741882801055908\n",
            "Val loss: 0.4695702499813504, Val f1: 0.7750111818313599\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.36722913895334514\n",
            "Train loss: 0.36936848674501693\n",
            "Train loss: 0.3720246496654692\n",
            "Train loss: 0.3722383073398045\n",
            "Train loss: 0.37325205751827784\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.35975694486073084, Val f1: 0.8632498383522034\n",
            "Val loss: 0.35965258308819364, Val f1: 0.8640095591545105\n",
            "Val loss: 0.36047315001487734, Val f1: 0.8637550473213196\n",
            "Val loss: 0.36000493935176303, Val f1: 0.8639437556266785\n",
            "Val loss: 0.3602410823958261, Val f1: 0.864276111125946\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4674893683857388, Val f1: 0.7712664604187012\n",
            "Val loss: 0.4718853235244751, Val f1: 0.7690260410308838\n",
            "Val loss: 0.47303846036946334, Val f1: 0.7700768709182739\n",
            "Val loss: 0.4703019989861382, Val f1: 0.771879255771637\n",
            "Val loss: 0.4675045821401808, Val f1: 0.7729957103729248\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.3628288882119315\n",
            "Train loss: 0.35938337658132824\n",
            "Train loss: 0.3607318103313446\n",
            "Train loss: 0.3632980485047613\n",
            "Train loss: 0.36498011333601815\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3517559255872454, Val f1: 0.8731523156166077\n",
            "Val loss: 0.3528617386307035, Val f1: 0.8702555298805237\n",
            "Val loss: 0.35310629180499487, Val f1: 0.869652271270752\n",
            "Val loss: 0.35379710282598226, Val f1: 0.8690130114555359\n",
            "Val loss: 0.35320348228727066, Val f1: 0.8694965243339539\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.46877779563268024, Val f1: 0.776919960975647\n",
            "Val loss: 0.47347162829505074, Val f1: 0.7727175951004028\n",
            "Val loss: 0.47457946671379936, Val f1: 0.772566556930542\n",
            "Val loss: 0.47157496876186794, Val f1: 0.7741042375564575\n",
            "Val loss: 0.4691007806195153, Val f1: 0.7756423950195312\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.3545044754232679\n",
            "Train loss: 0.3565439943756376\n",
            "Train loss: 0.3560749766372499\n",
            "Train loss: 0.35664578314338413\n",
            "Train loss: 0.35760688883917674\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34820525135312763, Val f1: 0.8723033666610718\n",
            "Val loss: 0.34762248396873474, Val f1: 0.872330367565155\n",
            "Val loss: 0.346828487941197, Val f1: 0.8733763098716736\n",
            "Val loss: 0.34691579959222246, Val f1: 0.8734723925590515\n",
            "Val loss: 0.3470135738168444, Val f1: 0.8737403750419617\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4734090401066674, Val f1: 0.7770895957946777\n",
            "Val loss: 0.4774860358900494, Val f1: 0.7765259742736816\n",
            "Val loss: 0.47814646032121444, Val f1: 0.7764434218406677\n",
            "Val loss: 0.47523685461945003, Val f1: 0.7772148251533508\n",
            "Val loss: 0.4725573367542691, Val f1: 0.778007447719574\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.3518674910068512\n",
            "Train loss: 0.3481284703527178\n",
            "Train loss: 0.3488296985626221\n",
            "Train loss: 0.3497983855860574\n",
            "Train loss: 0.3510748325075422\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34106707998684477, Val f1: 0.8755017518997192\n",
            "Val loss: 0.34085493087768554, Val f1: 0.8770097494125366\n",
            "Val loss: 0.34066322417486283, Val f1: 0.8772657513618469\n",
            "Val loss: 0.3404732725449971, Val f1: 0.8773189187049866\n",
            "Val loss: 0.34043515494891574, Val f1: 0.877265989780426\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.47495244277848137, Val f1: 0.7774137258529663\n",
            "Val loss: 0.4793791191445457, Val f1: 0.7764999270439148\n",
            "Val loss: 0.4800688260131412, Val f1: 0.7765681743621826\n",
            "Val loss: 0.47694113271103966, Val f1: 0.7780563235282898\n",
            "Val loss: 0.4743521809577942, Val f1: 0.7786169648170471\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.34165504830224175\n",
            "Train loss: 0.3408101507595607\n",
            "Train loss: 0.3416161454859234\n",
            "Train loss: 0.3439352978553091\n",
            "Train loss: 0.34465748139790126\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3293852048260825, Val f1: 0.882688045501709\n",
            "Val loss: 0.32679572233131954, Val f1: 0.8834675550460815\n",
            "Val loss: 0.3280099982307071, Val f1: 0.882167398929596\n",
            "Val loss: 0.32804134999002726, Val f1: 0.8822437524795532\n",
            "Val loss: 0.32789025732449123, Val f1: 0.882491946220398\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.46867156359884476, Val f1: 0.7746795415878296\n",
            "Val loss: 0.4740397549337811, Val f1: 0.7720257639884949\n",
            "Val loss: 0.4751591384410858, Val f1: 0.7713194489479065\n",
            "Val loss: 0.4716062951419089, Val f1: 0.7718484997749329\n",
            "Val loss: 0.4685835414462619, Val f1: 0.7725946307182312\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.33163609419550216\n",
            "Train loss: 0.3349522079740252\n",
            "Train loss: 0.3342703220390138\n",
            "Train loss: 0.3365289845636913\n",
            "Train loss: 0.3383636026723044\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3317534097603389, Val f1: 0.8834486603736877\n",
            "Val loss: 0.32641405165195464, Val f1: 0.8853979110717773\n",
            "Val loss: 0.3261884587151664, Val f1: 0.8858202695846558\n",
            "Val loss: 0.3266589062554496, Val f1: 0.8853905200958252\n",
            "Val loss: 0.32710015569414413, Val f1: 0.88461834192276\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.47975674271583557, Val f1: 0.7774723172187805\n",
            "Val loss: 0.4844162066777547, Val f1: 0.7769994139671326\n",
            "Val loss: 0.484538847649539, Val f1: 0.7773977518081665\n",
            "Val loss: 0.4810676856173409, Val f1: 0.7779366374015808\n",
            "Val loss: 0.47806014815966286, Val f1: 0.7786447405815125\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3245285953794207\n",
            "Train loss: 0.3284358437572207\n",
            "Train loss: 0.3294286918072473\n",
            "Train loss: 0.33061629618917193\n",
            "Train loss: 0.3317221195357186\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3113863945007324, Val f1: 0.8917068243026733\n",
            "Val loss: 0.3132247158459255, Val f1: 0.8911162614822388\n",
            "Val loss: 0.312658037741979, Val f1: 0.8913108706474304\n",
            "Val loss: 0.3121791618210929, Val f1: 0.8907005190849304\n",
            "Val loss: 0.3130391035761152, Val f1: 0.88998943567276\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.47249410549799603, Val f1: 0.7721239924430847\n",
            "Val loss: 0.477310202187962, Val f1: 0.7706155180931091\n",
            "Val loss: 0.4777517131081334, Val f1: 0.7701060175895691\n",
            "Val loss: 0.4744540618525611, Val f1: 0.7717714309692383\n",
            "Val loss: 0.47141092088487413, Val f1: 0.7729016542434692\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.32288348249026705\n",
            "Train loss: 0.3255696309464318\n",
            "Train loss: 0.3254175853161585\n",
            "Train loss: 0.32599288608346666\n",
            "Train loss: 0.32658759900501794\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.31017723509243555, Val f1: 0.8895996809005737\n",
            "Val loss: 0.3073452817542212, Val f1: 0.8934035301208496\n",
            "Val loss: 0.30679950657345, Val f1: 0.8935758471488953\n",
            "Val loss: 0.3074730985930988, Val f1: 0.8931430578231812\n",
            "Val loss: 0.30752430302756173, Val f1: 0.8929023146629333\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.47561686899926925, Val f1: 0.772493839263916\n",
            "Val loss: 0.4810202386644151, Val f1: 0.770778477191925\n",
            "Val loss: 0.4816713333129883, Val f1: 0.7707200050354004\n",
            "Val loss: 0.4778068893485599, Val f1: 0.7727552652359009\n",
            "Val loss: 0.47474909954600863, Val f1: 0.7736160755157471\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.31124490925243925\n",
            "Train loss: 0.3136927285364696\n",
            "Train loss: 0.31659716027123586\n",
            "Train loss: 0.31856630189078194\n",
            "Train loss: 0.3194086834362575\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3005259300981249, Val f1: 0.896479606628418\n",
            "Val loss: 0.30312482374055044, Val f1: 0.8948104381561279\n",
            "Val loss: 0.3048634568850199, Val f1: 0.8943379521369934\n",
            "Val loss: 0.3049800293786185, Val f1: 0.8947467803955078\n",
            "Val loss: 0.3052470290660858, Val f1: 0.8939484357833862\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.48463794589042664, Val f1: 0.7734061479568481\n",
            "Val loss: 0.4898342142502467, Val f1: 0.7728736996650696\n",
            "Val loss: 0.4898724114453351, Val f1: 0.7743236422538757\n",
            "Val loss: 0.48589369903008145, Val f1: 0.7754572033882141\n",
            "Val loss: 0.48300450377994114, Val f1: 0.7761211395263672\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.3063125848770142\n",
            "Train loss: 0.30861564193453106\n",
            "Train loss: 0.3127792199452718\n",
            "Train loss: 0.31462386335645404\n",
            "Train loss: 0.3148468003954206\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.29115157808576314, Val f1: 0.9014819860458374\n",
            "Val loss: 0.29282303409917015, Val f1: 0.9004339575767517\n",
            "Val loss: 0.2943770033972604, Val f1: 0.8994629383087158\n",
            "Val loss: 0.2950555086135864, Val f1: 0.8989695310592651\n",
            "Val loss: 0.2950295972824097, Val f1: 0.8989658951759338\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4823545747333103, Val f1: 0.7745831608772278\n",
            "Val loss: 0.4876370032628377, Val f1: 0.7726734280586243\n",
            "Val loss: 0.48789975488627396, Val f1: 0.7718643546104431\n",
            "Val loss: 0.48390838421053356, Val f1: 0.7733582854270935\n",
            "Val loss: 0.4808059268527561, Val f1: 0.7744438648223877\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.3047701895236969\n",
            "Train loss: 0.3036486097744533\n",
            "Train loss: 0.30612609556743076\n",
            "Train loss: 0.30894853004387446\n",
            "Train loss: 0.31097454616001674\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2989710143634251, Val f1: 0.8939307332038879\n",
            "Val loss: 0.2971765624625342, Val f1: 0.8963322639465332\n",
            "Val loss: 0.2977602061771211, Val f1: 0.8967196941375732\n",
            "Val loss: 0.2972062851701464, Val f1: 0.8974062204360962\n",
            "Val loss: 0.2974811555658068, Val f1: 0.8972143530845642\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4957159823841519, Val f1: 0.7739059925079346\n",
            "Val loss: 0.5009744200441573, Val f1: 0.7741405963897705\n",
            "Val loss: 0.5000504310484286, Val f1: 0.7756919860839844\n",
            "Val loss: 0.49594419697920483, Val f1: 0.7768537998199463\n",
            "Val loss: 0.4931435419453515, Val f1: 0.7770506143569946\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.3014151726450239\n",
            "Train loss: 0.30349000351769584\n",
            "Train loss: 0.30500225793747676\n",
            "Train loss: 0.30512483758585796\n",
            "Train loss: 0.3059943905898503\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28751226493290494, Val f1: 0.9032651782035828\n",
            "Val loss: 0.287851334469659, Val f1: 0.9032841920852661\n",
            "Val loss: 0.28729652648880366, Val f1: 0.9036303162574768\n",
            "Val loss: 0.287040162725108, Val f1: 0.9035325646400452\n",
            "Val loss: 0.28712566580091203, Val f1: 0.9033158421516418\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4927389489279853, Val f1: 0.7736631035804749\n",
            "Val loss: 0.49764781031343674, Val f1: 0.7734717130661011\n",
            "Val loss: 0.49701978983702483, Val f1: 0.774168848991394\n",
            "Val loss: 0.4928033236000273, Val f1: 0.7756727337837219\n",
            "Val loss: 0.49008670979075963, Val f1: 0.7763186097145081\n",
            "\n",
            "starting Epoch 25\n",
            "Training...\n",
            "Train loss: 0.29991670761789596\n",
            "Train loss: 0.298690664768219\n",
            "Train loss: 0.2984119937533424\n",
            "Train loss: 0.29977924547025137\n",
            "Train loss: 0.30227463245391845\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2855628958770207, Val f1: 0.9020058512687683\n",
            "Val loss: 0.283343916279929, Val f1: 0.904056966304779\n",
            "Val loss: 0.28194262867882136, Val f1: 0.904850423336029\n",
            "Val loss: 0.28094807203326905, Val f1: 0.9056684970855713\n",
            "Val loss: 0.2811234062058585, Val f1: 0.9053708910942078\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.49612173106935287, Val f1: 0.7706124782562256\n",
            "Val loss: 0.5003676381376054, Val f1: 0.7697044610977173\n",
            "Val loss: 0.4997265173329247, Val f1: 0.7704169154167175\n",
            "Val loss: 0.49523721800910103, Val f1: 0.7725210189819336\n",
            "Val loss: 0.4923143492804633, Val f1: 0.772940993309021\n",
            "\n",
            "starting Epoch 26\n",
            "Training...\n",
            "Train loss: 0.2902278968266078\n",
            "Train loss: 0.29069671205111913\n",
            "Train loss: 0.29219882573400224\n",
            "Train loss: 0.2942210037793432\n",
            "Train loss: 0.2951596772670746\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2700497465474265, Val f1: 0.9113507866859436\n",
            "Val loss: 0.2694387137889862, Val f1: 0.9112210273742676\n",
            "Val loss: 0.27040383787382216, Val f1: 0.9104556441307068\n",
            "Val loss: 0.2714492050664766, Val f1: 0.9098784327507019\n",
            "Val loss: 0.2715936129433768, Val f1: 0.9097474813461304\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4943058987458547, Val f1: 0.7670537233352661\n",
            "Val loss: 0.49867598050170475, Val f1: 0.7669496536254883\n",
            "Val loss: 0.49866525552890917, Val f1: 0.7683416604995728\n",
            "Val loss: 0.4941066371070014, Val f1: 0.770499050617218\n",
            "Val loss: 0.49107043345769247, Val f1: 0.7709361910820007\n",
            "\n",
            "starting Epoch 27\n",
            "Training...\n",
            "Train loss: 0.28178509984697614\n",
            "Train loss: 0.28496509322098323\n",
            "Train loss: 0.2881005962689718\n",
            "Train loss: 0.2888591955815043\n",
            "Train loss: 0.2913642941202436\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2750158258846828, Val f1: 0.9088233709335327\n",
            "Val loss: 0.27609451753752573, Val f1: 0.9084975123405457\n",
            "Val loss: 0.27606186951909745, Val f1: 0.908454954624176\n",
            "Val loss: 0.27601134585482734, Val f1: 0.9084683656692505\n",
            "Val loss: 0.27542738897459845, Val f1: 0.9088406562805176\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5076355437437693, Val f1: 0.7723574638366699\n",
            "Val loss: 0.5122392541832395, Val f1: 0.7712487578392029\n",
            "Val loss: 0.5114871197276645, Val f1: 0.7728119492530823\n",
            "Val loss: 0.5068270183271832, Val f1: 0.7743175625801086\n",
            "Val loss: 0.5039924363295237, Val f1: 0.7744356393814087\n",
            "\n",
            "starting Epoch 28\n",
            "Training...\n",
            "Train loss: 0.2828648494822638\n",
            "Train loss: 0.28345222238983425\n",
            "Train loss: 0.2847705845321928\n",
            "Train loss: 0.2865800142288208\n",
            "Train loss: 0.2880529633590153\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.25683981137616296, Val f1: 0.9152829051017761\n",
            "Val loss: 0.25893441970859254, Val f1: 0.913527250289917\n",
            "Val loss: 0.2585703852630797, Val f1: 0.9135673642158508\n",
            "Val loss: 0.2592173484819276, Val f1: 0.9133197069168091\n",
            "Val loss: 0.2600310413326536, Val f1: 0.9133312702178955\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.500004768371582, Val f1: 0.7635589241981506\n",
            "Val loss: 0.5045101195573807, Val f1: 0.7635844349861145\n",
            "Val loss: 0.5047878148379149, Val f1: 0.7649143934249878\n",
            "Val loss: 0.4997715147005187, Val f1: 0.7675321698188782\n",
            "Val loss: 0.49649728073014154, Val f1: 0.768334150314331\n",
            "\n",
            "starting Epoch 29\n",
            "Training...\n",
            "Train loss: 0.27453545928001405\n",
            "Train loss: 0.27870899992329734\n",
            "Train loss: 0.2799057565984272\n",
            "Train loss: 0.28182244364704406\n",
            "Train loss: 0.2842778549875532\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2582327331815447, Val f1: 0.9179918766021729\n",
            "Val loss: 0.25751000962087084, Val f1: 0.9175480604171753\n",
            "Val loss: 0.2578468504406157, Val f1: 0.9174559712409973\n",
            "Val loss: 0.2580557966870921, Val f1: 0.9171746373176575\n",
            "Val loss: 0.25818707040378025, Val f1: 0.9173433780670166\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5063279238012102, Val f1: 0.7680761218070984\n",
            "Val loss: 0.5112558470831977, Val f1: 0.7666555643081665\n",
            "Val loss: 0.5111755298243629, Val f1: 0.7674323916435242\n",
            "Val loss: 0.5060892651478449, Val f1: 0.7693909406661987\n",
            "Val loss: 0.5029912034670512, Val f1: 0.7698676586151123\n",
            "\n",
            "starting Epoch 30\n",
            "Training...\n",
            "Train loss: 0.2718222281762532\n",
            "Train loss: 0.27488487511873244\n",
            "Train loss: 0.27794951654615857\n",
            "Train loss: 0.2790396756359509\n",
            "Train loss: 0.28133195349148343\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2533865966967174, Val f1: 0.9181932806968689\n",
            "Val loss: 0.25335944678102224, Val f1: 0.9179056286811829\n",
            "Val loss: 0.25370451297078817, Val f1: 0.9186022877693176\n",
            "Val loss: 0.25313821381756235, Val f1: 0.9188668727874756\n",
            "Val loss: 0.2527955208505903, Val f1: 0.9191650152206421\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5100567075941298, Val f1: 0.7669203877449036\n",
            "Val loss: 0.5153130872382058, Val f1: 0.7645090222358704\n",
            "Val loss: 0.5149340154948058, Val f1: 0.7662911415100098\n",
            "Val loss: 0.5095203196009, Val f1: 0.7682673335075378\n",
            "Val loss: 0.5063775102297465, Val f1: 0.7688116431236267\n",
            "\n",
            "starting Epoch 31\n",
            "Training...\n",
            "Train loss: 0.27012523753302436\n",
            "Train loss: 0.26999164138521464\n",
            "Train loss: 0.2743700826451892\n",
            "Train loss: 0.2749506241508893\n",
            "Train loss: 0.27628456916127886\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.25171375274658203, Val f1: 0.9188254475593567\n",
            "Val loss: 0.25040847063064575, Val f1: 0.9193646907806396\n",
            "Val loss: 0.24994856048197972, Val f1: 0.9193571209907532\n",
            "Val loss: 0.2495724112859794, Val f1: 0.9193685054779053\n",
            "Val loss: 0.24989231944084167, Val f1: 0.9193733930587769\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.518943727016449, Val f1: 0.7673622369766235\n",
            "Val loss: 0.5240338378482394, Val f1: 0.7664068341255188\n",
            "Val loss: 0.5235714581277635, Val f1: 0.7678230404853821\n",
            "Val loss: 0.5182559606101778, Val f1: 0.7701042890548706\n",
            "Val loss: 0.5148479594124689, Val f1: 0.7704525589942932\n",
            "\n",
            "starting Epoch 32\n",
            "Training...\n",
            "Train loss: 0.2685807743242809\n",
            "Train loss: 0.2675603566425187\n",
            "Train loss: 0.26786211416834876\n",
            "Train loss: 0.2704329332070691\n",
            "Train loss: 0.27253893886293684\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.24727155651365007, Val f1: 0.9211683869361877\n",
            "Val loss: 0.2453701044831957, Val f1: 0.9223711490631104\n",
            "Val loss: 0.24562259259678068, Val f1: 0.9222071170806885\n",
            "Val loss: 0.2452675170132092, Val f1: 0.9223544597625732\n",
            "Val loss: 0.24453535769666945, Val f1: 0.9227864146232605\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5189340710639954, Val f1: 0.7690304517745972\n",
            "Val loss: 0.524239546722836, Val f1: 0.7669726610183716\n",
            "Val loss: 0.5240477610517431, Val f1: 0.7674872875213623\n",
            "Val loss: 0.518938740922345, Val f1: 0.769097626209259\n",
            "Val loss: 0.5154844131734636, Val f1: 0.7692820429801941\n",
            "\n",
            "starting Epoch 33\n",
            "Training...\n",
            "Train loss: 0.2613883912563324\n",
            "Train loss: 0.2668330692819187\n",
            "Train loss: 0.2685470388049171\n",
            "Train loss: 0.2687661037913391\n",
            "Train loss: 0.26931548859391896\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2394663917166846, Val f1: 0.9254504442214966\n",
            "Val loss: 0.23751677657876696, Val f1: 0.926924467086792\n",
            "Val loss: 0.23892598067011153, Val f1: 0.9252352118492126\n",
            "Val loss: 0.23924035866345678, Val f1: 0.9246523380279541\n",
            "Val loss: 0.23896178322178976, Val f1: 0.925214946269989\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5219312343332503, Val f1: 0.7648311257362366\n",
            "Val loss: 0.526876418126954, Val f1: 0.7646666765213013\n",
            "Val loss: 0.5270191221325485, Val f1: 0.766325056552887\n",
            "Val loss: 0.5216264807515674, Val f1: 0.7678547501564026\n",
            "Val loss: 0.5180866950088077, Val f1: 0.7687166929244995\n",
            "\n",
            "starting Epoch 34\n",
            "Training...\n",
            "Train loss: 0.2600637951067516\n",
            "Train loss: 0.2608876266649791\n",
            "Train loss: 0.2631028456347329\n",
            "Train loss: 0.26533339992165567\n",
            "Train loss: 0.26731859173093525\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2338075590985162, Val f1: 0.9273310303688049\n",
            "Val loss: 0.23164566989455904, Val f1: 0.9283962845802307\n",
            "Val loss: 0.23337886943703606, Val f1: 0.9272841811180115\n",
            "Val loss: 0.2344010684107031, Val f1: 0.9269918203353882\n",
            "Val loss: 0.23450452625751494, Val f1: 0.926918089389801\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5280711750189463, Val f1: 0.7661593556404114\n",
            "Val loss: 0.533620630701383, Val f1: 0.7643939852714539\n",
            "Val loss: 0.5331563673637532, Val f1: 0.7666958570480347\n",
            "Val loss: 0.5273343953821394, Val f1: 0.7684624195098877\n",
            "Val loss: 0.5235775185955895, Val f1: 0.7689608335494995\n",
            "\n",
            "starting Epoch 35\n",
            "Training...\n",
            "Train loss: 0.2573430687189102\n",
            "Train loss: 0.2591826745441982\n",
            "Train loss: 0.25925392380782536\n",
            "Train loss: 0.26048264556697437\n",
            "Train loss: 0.26216990837029047\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.22916705480643682, Val f1: 0.9298007488250732\n",
            "Val loss: 0.2310368503843035, Val f1: 0.9289724230766296\n",
            "Val loss: 0.22999671796957652, Val f1: 0.929043710231781\n",
            "Val loss: 0.23028033609901155, Val f1: 0.9287086725234985\n",
            "Val loss: 0.22979682053838457, Val f1: 0.9287953972816467\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5327921112378439, Val f1: 0.7631908059120178\n",
            "Val loss: 0.5374302433596717, Val f1: 0.7616555094718933\n",
            "Val loss: 0.5371483895513747, Val f1: 0.7634814977645874\n",
            "Val loss: 0.5311069240172704, Val f1: 0.7655298709869385\n",
            "Val loss: 0.5273630817731222, Val f1: 0.7663710117340088\n",
            "\n",
            "starting Epoch 36\n",
            "Training...\n",
            "Train loss: 0.2510461087737765\n",
            "Train loss: 0.25411915693964277\n",
            "Train loss: 0.255995988987741\n",
            "Train loss: 0.25907404997519085\n",
            "Train loss: 0.25898157170840674\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2286661216190883, Val f1: 0.9300829172134399\n",
            "Val loss: 0.22926035353115626, Val f1: 0.9298689961433411\n",
            "Val loss: 0.229089998063587, Val f1: 0.9300694465637207\n",
            "Val loss: 0.22995339057275227, Val f1: 0.9296006560325623\n",
            "Val loss: 0.23019838750362395, Val f1: 0.9292383790016174\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5419549610879686, Val f1: 0.7671862244606018\n",
            "Val loss: 0.5460967885123359, Val f1: 0.7652489542961121\n",
            "Val loss: 0.544971376657486, Val f1: 0.7668566703796387\n",
            "Val loss: 0.5390182990166876, Val f1: 0.7688590288162231\n",
            "Val loss: 0.5356124877929688, Val f1: 0.7696176767349243\n",
            "\n",
            "starting Epoch 37\n",
            "Training...\n",
            "Train loss: 0.2527957005160196\n",
            "Train loss: 0.25319227427244184\n",
            "Train loss: 0.2546153761091686\n",
            "Train loss: 0.2553534309778895\n",
            "Train loss: 0.2571860085214887\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2206694394350052, Val f1: 0.931960940361023\n",
            "Val loss: 0.22241849622556142, Val f1: 0.9311883449554443\n",
            "Val loss: 0.22145026013964697, Val f1: 0.9317665100097656\n",
            "Val loss: 0.2214722532246794, Val f1: 0.9313346743583679\n",
            "Val loss: 0.2215873350415911, Val f1: 0.9309923052787781\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5432951847712199, Val f1: 0.7590466737747192\n",
            "Val loss: 0.5472706390751733, Val f1: 0.7586432695388794\n",
            "Val loss: 0.5474908616807725, Val f1: 0.7588438391685486\n",
            "Val loss: 0.540533733036783, Val f1: 0.7611101865768433\n",
            "Val loss: 0.536784456173579, Val f1: 0.7612720131874084\n",
            "\n",
            "starting Epoch 38\n",
            "Training...\n",
            "Train loss: 0.24552388616970608\n",
            "Train loss: 0.24760113188198635\n",
            "Train loss: 0.249968644125121\n",
            "Train loss: 0.25071400393332754\n",
            "Train loss: 0.25175527138369425\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.21978126338550022, Val f1: 0.9342013597488403\n",
            "Val loss: 0.22184508804764067, Val f1: 0.9323805570602417\n",
            "Val loss: 0.22076467020171028, Val f1: 0.9325321912765503\n",
            "Val loss: 0.2201107103909765, Val f1: 0.9329206347465515\n",
            "Val loss: 0.21975607454776763, Val f1: 0.9328535795211792\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5518008271853129, Val f1: 0.7662819027900696\n",
            "Val loss: 0.5556084877914853, Val f1: 0.7637633681297302\n",
            "Val loss: 0.5544119322741473, Val f1: 0.7649891972541809\n",
            "Val loss: 0.5476660438709788, Val f1: 0.7672795653343201\n",
            "Val loss: 0.5440635674529606, Val f1: 0.7674027681350708\n",
            "\n",
            "starting Epoch 39\n",
            "Training...\n",
            "Train loss: 0.24120733950819287\n",
            "Train loss: 0.24600778605256762\n",
            "Train loss: 0.24699355037439438\n",
            "Train loss: 0.24876710485134806\n",
            "Train loss: 0.24981010760579792\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.21758917016642434, Val f1: 0.9348751902580261\n",
            "Val loss: 0.21742491253784724, Val f1: 0.9345900416374207\n",
            "Val loss: 0.21630113905384427, Val f1: 0.9346238374710083\n",
            "Val loss: 0.2162757139120783, Val f1: 0.9345489740371704\n",
            "Val loss: 0.21535660079547336, Val f1: 0.9350829124450684\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5554821756150987, Val f1: 0.7685915231704712\n",
            "Val loss: 0.5605475372738309, Val f1: 0.7649434208869934\n",
            "Val loss: 0.5595342737657053, Val f1: 0.7666913270950317\n",
            "Val loss: 0.552389839457141, Val f1: 0.76889967918396\n",
            "Val loss: 0.5483900374836392, Val f1: 0.7689697742462158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тоже переобучилась, но теперь минимальный val score 0,46... на 12 эпохе. Допустим, я на 13й эпохе остановаилась (можно не перезапускать...) Модель улучшилась."
      ],
      "metadata": {
        "id": "G0HDwgjmVAFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.title('Как изменяется loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Losses')\n",
        "plt.grid()\n",
        "ax.plot(losses, label='Trainig loss')\n",
        "ax.plot(losses_eval, label='Evaluation loss')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "3hpmKrLAVc4Y",
        "outputId": "74e0b512-4000-4488-b137-c4d464597be4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.title('Как изменяется F1')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Losses')\n",
        "plt.grid()\n",
        "ax.plot(f1s, label='Train F1')\n",
        "ax.plot(f1s_eval, label='Evaluation F1')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "iNXqsgcxV6R5",
        "outputId": "947256a5-a275-420d-d0cc-09fa35bb5e79"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как сильно улучшить: работать не только со словами, но и с символами, больше внимания уделять пунктуации, из которой состоят смайлики, которые указывают на настроение текста. См. вторую модель."
      ],
      "metadata": {
        "id": "JhO3MoAobEIQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4RBfeH7_qke"
      },
      "source": [
        "## Вторая модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Sf6IDAr_qke",
        "outputId": "ed62cc75-29a1-4acf-ea4f-4d2bdb2ccd48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 456\n",
            "уникальных символов, втретившихся больше 5 раз: 234\n"
          ]
        }
      ],
      "source": [
        "# Делаем symbol2id и id2symbol\n",
        "\n",
        "vocab_s = Counter()\n",
        "for text in twits['text']:\n",
        "    vocab_s.update(list(text))\n",
        "print('всего уникальных символов:', len(vocab_s))\n",
        "filtered_vocab_s = set()\n",
        "\n",
        "for symbol in vocab_s:\n",
        "    if vocab_s[symbol] > 5:\n",
        "        filtered_vocab_s.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_s))\n",
        "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab_s:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 988
        },
        "id": "8lWtUTqC_qkf",
        "outputId": "b45e39b6-2f36-474e-ced3-29aef4286b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 226834/226834 [00:02<00:00, 77599.37it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-49bb47a5-6ad1-4917-9ceb-f37f061ea5fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>name</th>\n",
              "      <th>text</th>\n",
              "      <th>type</th>\n",
              "      <th>rep</th>\n",
              "      <th>rtw</th>\n",
              "      <th>fav</th>\n",
              "      <th>stcount</th>\n",
              "      <th>foll</th>\n",
              "      <th>frien</th>\n",
              "      <th>listcount</th>\n",
              "      <th>sent</th>\n",
              "      <th>words</th>\n",
              "      <th>symbols</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906692374446080</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>pleease_shut_up</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7569</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[хоть, я, и, школотый, но, поверь, у, мы, то, ...</td>\n",
              "      <td>[@, f, i, r, s, t, _, t, i, m, e, e,  , х, о, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906692693221377</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>alinakirpicheva</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11825</td>\n",
              "      <td>59</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>[да, он, немного, похожий, на, он, но, мой, ма...</td>\n",
              "      <td>[Д, а, ,,  , в, с, е, -, т, а, к, и,  , о, н, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>408906695083954177</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>EvgeshaRe</td>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1273</td>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[rt, katiacheh, ну, ты, идиотка, я, испугаться...</td>\n",
              "      <td>[R, T,  , @, K, a, t, i, a, C, h, e, h, :,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>408906695356973056</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>ikonnikova_21</td>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1549</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[rt, кто, то, в, угол, сидеть, и, погибать, от...</td>\n",
              "      <td>[R, T,  , @, d, i, g, g, e, r, 2, 9, 1, 2, :, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>408906761416867842</td>\n",
              "      <td>1386325943</td>\n",
              "      <td>JumpyAlex</td>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>597</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[вот, что, значит, страшилка, d, но, блин, пос...</td>\n",
              "      <td>[@, i, r, i, n, a, _, d, y, s, h, k, a, n, t, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226829</th>\n",
              "      <td>425138243257253888</td>\n",
              "      <td>1390195830</td>\n",
              "      <td>Yanch_96</td>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1138</td>\n",
              "      <td>32</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[но, не, каждый, хотеть, что, то, исправлять, ...</td>\n",
              "      <td>[Н, о,  , н, е,  , к, а, ж, д, ы, й,  , х, о, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226830</th>\n",
              "      <td>425138339503943682</td>\n",
              "      <td>1390195853</td>\n",
              "      <td>tkit_on</td>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4822</td>\n",
              "      <td>38</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[скучать, так, только, taaannyaaa, вправлять, ...</td>\n",
              "      <td>[с, к, у, ч, а, ю,  , т, а, к,  , :, -, (,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226831</th>\n",
              "      <td>425138437684215808</td>\n",
              "      <td>1390195876</td>\n",
              "      <td>ckooker1</td>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[вот, и, в, школа, в, говно, это, идти, уже, н...</td>\n",
              "      <td>[В, о, т,  , и,  , в,  , ш, к, о, л, у, ,,  , ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226832</th>\n",
              "      <td>425138490452344832</td>\n",
              "      <td>1390195889</td>\n",
              "      <td>LisaBeroud</td>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2516</td>\n",
              "      <td>187</td>\n",
              "      <td>265</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[rt, lisaberoud, тауриэль, не, грусть]</td>\n",
              "      <td>[R, T,  , @, _, T, h, e, m, _, _, :,  , @, L, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226833</th>\n",
              "      <td>425138595251625984</td>\n",
              "      <td>1390195914</td>\n",
              "      <td>sukapavlov</td>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7778</td>\n",
              "      <td>146</td>\n",
              "      <td>66</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>[такси, везти, я, на, работа, раздумывать, при...</td>\n",
              "      <td>[Т, а, к, с, и,  , в, е, з, е, т,  , м, е, н, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226834 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49bb47a5-6ad1-4917-9ceb-f37f061ea5fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-49bb47a5-6ad1-4917-9ceb-f37f061ea5fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-49bb47a5-6ad1-4917-9ceb-f37f061ea5fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        id  ...                                            symbols\n",
              "0       408906692374446080  ...  [@, f, i, r, s, t, _, t, i, m, e, e,  , х, о, ...\n",
              "1       408906692693221377  ...  [Д, а, ,,  , в, с, е, -, т, а, к, и,  , о, н, ...\n",
              "2       408906695083954177  ...  [R, T,  , @, K, a, t, i, a, C, h, e, h, :,  , ...\n",
              "3       408906695356973056  ...  [R, T,  , @, d, i, g, g, e, r, 2, 9, 1, 2, :, ...\n",
              "4       408906761416867842  ...  [@, i, r, i, n, a, _, d, y, s, h, k, a, n, t, ...\n",
              "...                    ...  ...                                                ...\n",
              "226829  425138243257253888  ...  [Н, о,  , н, е,  , к, а, ж, д, ы, й,  , х, о, ...\n",
              "226830  425138339503943682  ...  [с, к, у, ч, а, ю,  , т, а, к,  , :, -, (,  , ...\n",
              "226831  425138437684215808  ...  [В, о, т,  , и,  , в,  , ш, к, о, л, у, ,,  , ...\n",
              "226832  425138490452344832  ...  [R, T,  , @, _, T, h, e, m, _, _, :,  , @, L, ...\n",
              "226833  425138595251625984  ...  [Т, а, к, с, и,  , в, е, з, е, т,  , м, е, н, ...\n",
              "\n",
              "[226834 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "twits['symbols'] = twits['text'].progress_apply(lambda x: list(x))\n",
        "twits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_ofTvsiO_qkf",
        "outputId": "0fabf95c-51e1-4f3e-a2ba-bc8c2a5a5c18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b53a6589-4d85-4db8-9aad-91972d6c522f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>symbols</th>\n",
              "      <th>sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[хоть, я, и, школотый, но, поверь, у, мы, то, ...</td>\n",
              "      <td>[@, f, i, r, s, t, _, t, i, m, e, e,  , х, о, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[да, он, немного, похожий, на, он, но, мой, ма...</td>\n",
              "      <td>[Д, а, ,,  , в, с, е, -, т, а, к, и,  , о, н, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[rt, katiacheh, ну, ты, идиотка, я, испугаться...</td>\n",
              "      <td>[R, T,  , @, K, a, t, i, a, C, h, e, h, :,  , ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[rt, кто, то, в, угол, сидеть, и, погибать, от...</td>\n",
              "      <td>[R, T,  , @, d, i, g, g, e, r, 2, 9, 1, 2, :, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[вот, что, значит, страшилка, d, но, блин, пос...</td>\n",
              "      <td>[@, i, r, i, n, a, _, d, y, s, h, k, a, n, t, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226829</th>\n",
              "      <td>[но, не, каждый, хотеть, что, то, исправлять, ...</td>\n",
              "      <td>[Н, о,  , н, е,  , к, а, ж, д, ы, й,  , х, о, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226830</th>\n",
              "      <td>[скучать, так, только, taaannyaaa, вправлять, ...</td>\n",
              "      <td>[с, к, у, ч, а, ю,  , т, а, к,  , :, -, (,  , ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226831</th>\n",
              "      <td>[вот, и, в, школа, в, говно, это, идти, уже, н...</td>\n",
              "      <td>[В, о, т,  , и,  , в,  , ш, к, о, л, у, ,,  , ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226832</th>\n",
              "      <td>[rt, lisaberoud, тауриэль, не, грусть]</td>\n",
              "      <td>[R, T,  , @, _, T, h, e, m, _, _, :,  , @, L, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226833</th>\n",
              "      <td>[такси, везти, я, на, работа, раздумывать, при...</td>\n",
              "      <td>[Т, а, к, с, и,  , в, е, з, е, т,  , м, е, н, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>226834 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b53a6589-4d85-4db8-9aad-91972d6c522f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b53a6589-4d85-4db8-9aad-91972d6c522f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b53a6589-4d85-4db8-9aad-91972d6c522f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    words  ... sent\n",
              "0       [хоть, я, и, школотый, но, поверь, у, мы, то, ...  ...    1\n",
              "1       [да, он, немного, похожий, на, он, но, мой, ма...  ...    1\n",
              "2       [rt, katiacheh, ну, ты, идиотка, я, испугаться...  ...    1\n",
              "3       [rt, кто, то, в, угол, сидеть, и, погибать, от...  ...    1\n",
              "4       [вот, что, значит, страшилка, d, но, блин, пос...  ...    1\n",
              "...                                                   ...  ...  ...\n",
              "226829  [но, не, каждый, хотеть, что, то, исправлять, ...  ...    0\n",
              "226830  [скучать, так, только, taaannyaaa, вправлять, ...  ...    0\n",
              "226831  [вот, и, в, школа, в, говно, это, идти, уже, н...  ...    0\n",
              "226832             [rt, lisaberoud, тауриэль, не, грусть]  ...    0\n",
              "226833  [такси, везти, я, на, работа, раздумывать, при...  ...    0\n",
              "\n",
              "[226834 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "data2 = twits[['words', 'symbols', 'sent']]\n",
        "data2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "USqB6wPo_qkg"
      },
      "outputs": [],
      "source": [
        "class TwitDataset2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset_w = dataset['words'].values\n",
        "        self.word2id = word2id\n",
        "        self.dataset_s = dataset['symbols'].values\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['sent'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = list(self.dataset_w[index])\n",
        "        symbols = list(self.dataset_s[index])\n",
        "        ids_w = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        ids_s = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids_w, ids_s, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "        ids_w, ids_s, y = list(zip(*batch))\n",
        "        padded_ids_w = pad_sequence(ids_w, batch_first=True).to(self.device)\n",
        "        padded_ids_s = pad_sequence(ids_s, batch_first=True).to(self.device)\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        \n",
        "        return padded_ids_w, padded_ids_s, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yJoJe7-8_qkg"
      },
      "outputs": [],
      "source": [
        "train_data2, val_data2 = train_test_split(data2, test_size=0.2)\n",
        "train_dataset2 = TwitDataset2(train_data2, word2id, symbol2id, DEVICE)\n",
        "train_sampler2 = RandomSampler(train_dataset2)\n",
        "train_iterator2 = DataLoader(train_dataset2, collate_fn = train_dataset2.collate_fn,\n",
        "                            sampler=train_sampler2, batch_size=1024)\n",
        "#train_data['sent'].values\n",
        "batch = next(iter(train_iterator2))\n",
        "# batch[0].shape\n",
        "# [id2word[int(i)] for i in batch[0][0]]\n",
        "# batch[1]\n",
        "val_dataset2 = TwitDataset2(val_data2, word2id, symbol2id, DEVICE)\n",
        "val_sampler2 = SequentialSampler(val_dataset2)\n",
        "val_iterator2 = DataLoader(val_dataset2, collate_fn = val_dataset2.collate_fn,\n",
        "                          sampler=val_sampler2, batch_size=1024)\n",
        "val_batch = next(iter(val_iterator2))\n",
        "#val_batch[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "qC6dqwd7_qkg"
      },
      "outputs": [],
      "source": [
        "class CNN_2(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size_w, embedding_dim_w, vocab_size_s, embedding_dim_s):\n",
        "        super().__init__()\n",
        "        self.embedding_w = nn.Embedding(vocab_size_w, embedding_dim_w)\n",
        "        self.embedding_s = nn.Embedding(vocab_size_s, embedding_dim_s)\n",
        "        self.hid_for_senten = nn.Linear(in_features=embedding_dim_w, out_features=embedding_dim_w//2)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim_s, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim_s, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180+embedding_dim_w//2, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, padded_ids_w, padded_ids_s):\n",
        "        words = padded_ids_w\n",
        "        symbols = padded_ids_s\n",
        "        \n",
        "        #batch_size x seq_len\n",
        "        embedded_w = self.embedding_w(words)\n",
        "        embedded_sentence = torch.mean(embedded_w, dim=1)\n",
        "        vec_X = self.hid_for_senten(embedded_sentence)\n",
        "        \n",
        "        #batch_size x seq_len\n",
        "        embedded_s = self.embedding_s(symbols)\n",
        "        \n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded_s = embedded_s.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded_s))))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded_s))))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        # batch_size x filter_count2\n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((pooling1, pooling2, vec_X), 1)\n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Ygh9qj41_qkh"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем,\n",
        "                   # что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (words, symbols, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(words, symbols)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/(i+1)}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "T-rbkSyM_qkj"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (words, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(words, symbols)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/(i+1)}, Val f1: {epoch_metric/(i+1)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)\n",
        "                                   # возвращаем среднее значение по всей выборке"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_2(len(word2id), 186, len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "hmgZ-rc-afI0"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ojQSjqs_qkk",
        "outputId": "b13df8b1-8d6e-4233-a65c-c8c8c061aceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.65715685571943\n",
            "Train loss: 0.6202949575015477\n",
            "Train loss: 0.5789406251339685\n",
            "Train loss: 0.5333271335278239\n",
            "Train loss: 0.4873780977725983\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3116850427218846, Val f1: 0.9082210063934326\n",
            "Val loss: 0.31212810235364097, Val f1: 0.9070793986320496\n",
            "Val loss: 0.31253594358762105, Val f1: 0.9062703251838684\n",
            "Val loss: 0.31335519062621253, Val f1: 0.90558260679245\n",
            "Val loss: 0.31341134326798575, Val f1: 0.9057555794715881\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3129340178436703, Val f1: 0.9067867398262024\n",
            "Val loss: 0.31431927780310315, Val f1: 0.9064907431602478\n",
            "Val loss: 0.3140040625024725, Val f1: 0.9054238200187683\n",
            "Val loss: 0.3139054874579112, Val f1: 0.9057860970497131\n",
            "Val loss: 0.31367385851012336, Val f1: 0.9070649147033691\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.2237735458782741\n",
            "Train loss: 0.19489643062864032\n",
            "Train loss: 0.17123624568893797\n",
            "Train loss: 0.15253119207918645\n",
            "Train loss: 0.13709900592054639\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.07545283543212074, Val f1: 0.9866423606872559\n",
            "Val loss: 0.07683363599436624, Val f1: 0.9852744936943054\n",
            "Val loss: 0.07691845347483953, Val f1: 0.9851507544517517\n",
            "Val loss: 0.07691201157867908, Val f1: 0.9851875305175781\n",
            "Val loss: 0.07710760018655231, Val f1: 0.9852038621902466\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.07566078752279282, Val f1: 0.9853497743606567\n",
            "Val loss: 0.0767439670032925, Val f1: 0.9855009913444519\n",
            "Val loss: 0.07706933965285619, Val f1: 0.9854419231414795\n",
            "Val loss: 0.07726714946329594, Val f1: 0.9857266545295715\n",
            "Val loss: 0.0772335966428121, Val f1: 0.9855508804321289\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.05993575123803956\n",
            "Train loss: 0.05465990374130862\n",
            "Train loss: 0.05120102532562756\n",
            "Train loss: 0.04764409480350358\n",
            "Train loss: 0.044746958964637346\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.03627028475914683, Val f1: 0.9914731979370117\n",
            "Val loss: 0.036501891325627055, Val f1: 0.9915470480918884\n",
            "Val loss: 0.03648813162885961, Val f1: 0.9917210340499878\n",
            "Val loss: 0.03648419420101813, Val f1: 0.9918153882026672\n",
            "Val loss: 0.03622338305626597, Val f1: 0.9919645190238953\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.035585002352794014, Val f1: 0.9923815131187439\n",
            "Val loss: 0.036145602042476334, Val f1: 0.9924829602241516\n",
            "Val loss: 0.0363512046083256, Val f1: 0.9921107888221741\n",
            "Val loss: 0.036583164137684636, Val f1: 0.9921666383743286\n",
            "Val loss: 0.0364829842829042, Val f1: 0.9920606017112732\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.029322032683662007\n",
            "Train loss: 0.028299972494798046\n",
            "Train loss: 0.02709420094532626\n",
            "Train loss: 0.026472742057272364\n",
            "Train loss: 0.0254496211877891\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.02363972126373223, Val f1: 0.9949231743812561\n",
            "Val loss: 0.023239167486982685, Val f1: 0.9951207041740417\n",
            "Val loss: 0.023421250265978633, Val f1: 0.9949485659599304\n",
            "Val loss: 0.023607393926275626, Val f1: 0.9948675036430359\n",
            "Val loss: 0.023597963431051798, Val f1: 0.9949153065681458\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.023365378379821777, Val f1: 0.9948532581329346\n",
            "Val loss: 0.02375531517383125, Val f1: 0.9950702786445618\n",
            "Val loss: 0.023935401467261492, Val f1: 0.9950015544891357\n",
            "Val loss: 0.02418828062299225, Val f1: 0.9949505925178528\n",
            "Val loss: 0.02398407513068782, Val f1: 0.9950605034828186\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.020188818579273564\n",
            "Train loss: 0.01948269185210977\n",
            "Train loss: 0.018979012904067834\n",
            "Train loss: 0.018608547607436778\n",
            "Train loss: 0.018246991581150465\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.018896576522716455, Val f1: 0.9954245686531067\n",
            "Val loss: 0.018758152851036616, Val f1: 0.9956297874450684\n",
            "Val loss: 0.018584379624752772, Val f1: 0.9957842230796814\n",
            "Val loss: 0.01863947858634804, Val f1: 0.995667040348053\n",
            "Val loss: 0.018795578474445, Val f1: 0.9956569671630859\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.01902961844785346, Val f1: 0.9949560165405273\n",
            "Val loss: 0.01936960830870602, Val f1: 0.9954536557197571\n",
            "Val loss: 0.019518127664923668, Val f1: 0.9954371452331543\n",
            "Val loss: 0.019773146861957178, Val f1: 0.9953582882881165\n",
            "Val loss: 0.019463318586349487, Val f1: 0.995497465133667\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.015699429836656367\n",
            "Train loss: 0.0154119896037238\n",
            "Train loss: 0.014958881675487474\n",
            "Train loss: 0.014602602040395141\n",
            "Train loss: 0.014419174071933542\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.014431532166366065, Val f1: 0.9964949488639832\n",
            "Val loss: 0.014411522587761284, Val f1: 0.9965872764587402\n",
            "Val loss: 0.014776868971862964, Val f1: 0.9964607954025269\n",
            "Val loss: 0.014398669341712125, Val f1: 0.9966355562210083\n",
            "Val loss: 0.014556491537285703, Val f1: 0.9965458512306213\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.015101897219816843, Val f1: 0.996149480342865\n",
            "Val loss: 0.01542324298578832, Val f1: 0.9962099194526672\n",
            "Val loss: 0.015574440325575846, Val f1: 0.9961239695549011\n",
            "Val loss: 0.015844626585021615, Val f1: 0.9960611462593079\n",
            "Val loss: 0.015521362879210049, Val f1: 0.9962133169174194\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.01240411140024662\n",
            "Train loss: 0.012736281286925078\n",
            "Train loss: 0.012235822590688865\n",
            "Train loss: 0.011978502558278186\n",
            "Train loss: 0.01176836896421654\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.011455943062901496, Val f1: 0.9966931939125061\n",
            "Val loss: 0.011348361176039492, Val f1: 0.9967847466468811\n",
            "Val loss: 0.011244690892774434, Val f1: 0.996833086013794\n",
            "Val loss: 0.0112174152695973, Val f1: 0.9968882203102112\n",
            "Val loss: 0.011078991336481912, Val f1: 0.9969682097434998\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.011990593332383368, Val f1: 0.9970106482505798\n",
            "Val loss: 0.01232448841134707, Val f1: 0.9968022108078003\n",
            "Val loss: 0.012475019769260177, Val f1: 0.9966287612915039\n",
            "Val loss: 0.012721757689076994, Val f1: 0.9966333508491516\n",
            "Val loss: 0.012443688563588592, Val f1: 0.9967834949493408\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.010589755259986434\n",
            "Train loss: 0.009990078643230454\n",
            "Train loss: 0.009874017603163207\n",
            "Train loss: 0.009774277628665524\n",
            "Train loss: 0.009679215236433916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.009997645073703357, Val f1: 0.9969374537467957\n",
            "Val loss: 0.009572513953649572, Val f1: 0.9972118139266968\n",
            "Val loss: 0.009925806274016699, Val f1: 0.9970127940177917\n",
            "Val loss: 0.009965400750349676, Val f1: 0.996964693069458\n",
            "Val loss: 0.00984709788912109, Val f1: 0.9970554709434509\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.011104732079224454, Val f1: 0.9967910051345825\n",
            "Val loss: 0.011451995802215405, Val f1: 0.9966385364532471\n",
            "Val loss: 0.011560929594216523, Val f1: 0.9964802861213684\n",
            "Val loss: 0.011793104272025326, Val f1: 0.9964677095413208\n",
            "Val loss: 0.011477853523360358, Val f1: 0.9965823888778687\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.008429942027266537\n",
            "Train loss: 0.00827975426800549\n",
            "Train loss: 0.008281027392617294\n",
            "Train loss: 0.008046640964624072\n",
            "Train loss: 0.007719753851581897\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.007545938608901842, Val f1: 0.9976024031639099\n",
            "Val loss: 0.007528685239542808, Val f1: 0.9975985884666443\n",
            "Val loss: 0.007658704694005705, Val f1: 0.9976035952568054\n",
            "Val loss: 0.007688557767375772, Val f1: 0.9975278377532959\n",
            "Val loss: 0.007551875714478748, Val f1: 0.9976456165313721\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.008991160636974705, Val f1: 0.9973288774490356\n",
            "Val loss: 0.009341822678430213, Val f1: 0.9972313642501831\n",
            "Val loss: 0.009416804855896367, Val f1: 0.9970962405204773\n",
            "Val loss: 0.009631104702647362, Val f1: 0.9970667362213135\n",
            "Val loss: 0.009392632512996595, Val f1: 0.9972183704376221\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.0065164029465190005\n",
            "Train loss: 0.006148973825786795\n",
            "Train loss: 0.005954631653037809\n",
            "Train loss: 0.005785470487483378\n",
            "Train loss: 0.005801648258098534\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.007011583893160735, Val f1: 0.9978183507919312\n",
            "Val loss: 0.006965829897671938, Val f1: 0.9977933764457703\n",
            "Val loss: 0.007366153695398853, Val f1: 0.9975872039794922\n",
            "Val loss: 0.007374420715495944, Val f1: 0.9976500272750854\n",
            "Val loss: 0.0073451881004231315, Val f1: 0.9976944327354431\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.008922837105476193, Val f1: 0.9970046281814575\n",
            "Val loss: 0.009207811827460924, Val f1: 0.9970632195472717\n",
            "Val loss: 0.00921212502375797, Val f1: 0.9970197677612305\n",
            "Val loss: 0.009428503535067042, Val f1: 0.9969261884689331\n",
            "Val loss: 0.009166422300040722, Val f1: 0.9970576167106628\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.004922577113445316\n",
            "Train loss: 0.004815594432875514\n",
            "Train loss: 0.004710467414753068\n",
            "Train loss: 0.004606028388453914\n",
            "Train loss: 0.00446924035237836\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00637866041756102, Val f1: 0.9981451630592346\n",
            "Val loss: 0.006702572825763906, Val f1: 0.9979462623596191\n",
            "Val loss: 0.006903220431524373, Val f1: 0.9978670477867126\n",
            "Val loss: 0.00684872737121103, Val f1: 0.9978865385055542\n",
            "Val loss: 0.006670156710648111, Val f1: 0.9979425072669983\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.00837342461778058, Val f1: 0.9971060156822205\n",
            "Val loss: 0.008590421277201839, Val f1: 0.9971133470535278\n",
            "Val loss: 0.008521580947907987, Val f1: 0.9972026348114014\n",
            "Val loss: 0.008725688805700176, Val f1: 0.9970634579658508\n",
            "Val loss: 0.008487588798420296, Val f1: 0.997211754322052\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.003627925439338599\n",
            "Train loss: 0.0036639201388295206\n",
            "Train loss: 0.0036564868508971162\n",
            "Train loss: 0.0035861823533196004\n",
            "Train loss: 0.003480907986605806\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.003934755116435034, Val f1: 0.9992645382881165\n",
            "Val loss: 0.0038077109094176976, Val f1: 0.9994111061096191\n",
            "Val loss: 0.0037960556912280266, Val f1: 0.9993784427642822\n",
            "Val loss: 0.0037201544660742263, Val f1: 0.999416708946228\n",
            "Val loss: 0.0037452698133087583, Val f1: 0.9993687272071838\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.005208412547492319, Val f1: 0.9988358616828918\n",
            "Val loss: 0.0053584796712837285, Val f1: 0.998462975025177\n",
            "Val loss: 0.005332153816534965, Val f1: 0.9985779523849487\n",
            "Val loss: 0.005455045663337741, Val f1: 0.9984509348869324\n",
            "Val loss: 0.00532521833665669, Val f1: 0.9985229969024658\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.0026988473920417683\n",
            "Train loss: 0.0029080045875161885\n",
            "Train loss: 0.002801612698073898\n",
            "Train loss: 0.002759981149574742\n",
            "Train loss: 0.0027294711349532008\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.003161410734589611, Val f1: 0.9996698498725891\n",
            "Val loss: 0.0031715531517485422, Val f1: 0.999600350856781\n",
            "Val loss: 0.003076570305884594, Val f1: 0.999630331993103\n",
            "Val loss: 0.0030781517279267843, Val f1: 0.9996057748794556\n",
            "Val loss: 0.0030915022741204927, Val f1: 0.9996081590652466\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.004505705191857285, Val f1: 0.9990535378456116\n",
            "Val loss: 0.0046219316750971805, Val f1: 0.9986813068389893\n",
            "Val loss: 0.004593787663098838, Val f1: 0.9988670349121094\n",
            "Val loss: 0.004698804463259876, Val f1: 0.9987505078315735\n",
            "Val loss: 0.0045968286382655306, Val f1: 0.998784601688385\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.002074322247478579\n",
            "Train loss: 0.002213770620125745\n",
            "Train loss: 0.002293747868610635\n",
            "Train loss: 0.0022924607104089644\n",
            "Train loss: 0.0023062983131967485\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0031609565052869064, Val f1: 0.9997538924217224\n",
            "Val loss: 0.002975693065673113, Val f1: 0.9996550679206848\n",
            "Val loss: 0.0029768581424529355, Val f1: 0.9996328353881836\n",
            "Val loss: 0.0029292208300570823, Val f1: 0.9996418356895447\n",
            "Val loss: 0.0029526279028505085, Val f1: 0.9996245503425598\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.004377373778778646, Val f1: 0.9987314343452454\n",
            "Val loss: 0.004458919995360904, Val f1: 0.9985201954841614\n",
            "Val loss: 0.004392025405885997, Val f1: 0.9987596869468689\n",
            "Val loss: 0.004508218630993118, Val f1: 0.9986699223518372\n",
            "Val loss: 0.004393026613009473, Val f1: 0.9987184405326843\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.001995992041858179\n",
            "Train loss: 0.0020296783826779575\n",
            "Train loss: 0.0020020026918722406\n",
            "Train loss: 0.0019836170129045576\n",
            "Train loss: 0.001972551456586059\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.001978867331386677, Val f1: 0.9997794032096863\n",
            "Val loss: 0.002089515719230154, Val f1: 0.9998214840888977\n",
            "Val loss: 0.0020963113045408613, Val f1: 0.9998153448104858\n",
            "Val loss: 0.002170554721461875, Val f1: 0.9998067617416382\n",
            "Val loss: 0.002187920003198087, Val f1: 0.9997904896736145\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.003477552375342283, Val f1: 0.9990535378456116\n",
            "Val loss: 0.003548236195153246, Val f1: 0.9990552067756653\n",
            "Val loss: 0.0035144057064489635, Val f1: 0.9991894364356995\n",
            "Val loss: 0.003607039249295162, Val f1: 0.9990462064743042\n",
            "Val loss: 0.0035245306841615175, Val f1: 0.9991297721862793\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.0018180322234651871\n",
            "Train loss: 0.001762959871224926\n",
            "Train loss: 0.0017554205706498275\n",
            "Train loss: 0.0017449884028922368\n",
            "Train loss: 0.0017236878900855248\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.002347110634270523, Val f1: 0.9997475743293762\n",
            "Val loss: 0.002249961775461478, Val f1: 0.9997629523277283\n",
            "Val loss: 0.0021846806214723203, Val f1: 0.9997961521148682\n",
            "Val loss: 0.002181967655113632, Val f1: 0.9997835755348206\n",
            "Val loss: 0.002167872808474515, Val f1: 0.9997822642326355\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.00347188097010884, Val f1: 0.9991548657417297\n",
            "Val loss: 0.003524649008694622, Val f1: 0.9992130994796753\n",
            "Val loss: 0.003462277085485834, Val f1: 0.9993299841880798\n",
            "Val loss: 0.0035614552228556326, Val f1: 0.9991516470909119\n",
            "Val loss: 0.0034785619497092234, Val f1: 0.999191403388977\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.0017370788463657455\n",
            "Train loss: 0.0015580865296734763\n",
            "Train loss: 0.0014450380367980826\n",
            "Train loss: 0.001429135093349032\n",
            "Train loss: 0.0014410703775605985\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0018873989083138959, Val f1: 0.9997785091400146\n",
            "Val loss: 0.0020787911556128945, Val f1: 0.9997517466545105\n",
            "Val loss: 0.002053811042458706, Val f1: 0.9997414350509644\n",
            "Val loss: 0.002023382871993817, Val f1: 0.9997708201408386\n",
            "Val loss: 0.001984970814748002, Val f1: 0.9997890591621399\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0032822779224564633, Val f1: 0.9992567896842957\n",
            "Val loss: 0.0033326479671005574, Val f1: 0.9992641806602478\n",
            "Val loss: 0.003267693492859878, Val f1: 0.9993640184402466\n",
            "Val loss: 0.0033678088382455623, Val f1: 0.9991772174835205\n",
            "Val loss: 0.003278076563340922, Val f1: 0.999211847782135\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.0011804971527973457\n",
            "Train loss: 0.0012722734968909728\n",
            "Train loss: 0.001264661351507086\n",
            "Train loss: 0.0012976280721237084\n",
            "Train loss: 0.0013057886582932303\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0010086482507176697, Val f1: 0.9999714493751526\n",
            "Val loss: 0.0009963469255516038, Val f1: 0.9999719858169556\n",
            "Val loss: 0.0010528516983391628, Val f1: 0.999954342842102\n",
            "Val loss: 0.0010813048225827514, Val f1: 0.9999307990074158\n",
            "Val loss: 0.0010904683654994836, Val f1: 0.9999337196350098\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0020379569614306092, Val f1: 0.999584436416626\n",
            "Val loss: 0.0020933672527058255, Val f1: 0.9997398853302002\n",
            "Val loss: 0.002132466841161389, Val f1: 0.9996814131736755\n",
            "Val loss: 0.0021465091089743916, Val f1: 0.9996281862258911\n",
            "Val loss: 0.0021086608046769267, Val f1: 0.9996377229690552\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.001220138107393203\n",
            "Train loss: 0.00111668668936805\n",
            "Train loss: 0.0011629024980634097\n",
            "Train loss: 0.0010715384057091016\n",
            "Train loss: 0.0010851552739872465\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.001549126789905131, Val f1: 0.9998347759246826\n",
            "Val loss: 0.0016912326383005296, Val f1: 0.9998759627342224\n",
            "Val loss: 0.0016060765003896363, Val f1: 0.9998804330825806\n",
            "Val loss: 0.0016184536896097208, Val f1: 0.9998491406440735\n",
            "Val loss: 0.0016008941508230887, Val f1: 0.9998627305030823\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.002834115850014819, Val f1: 0.9992453455924988\n",
            "Val loss: 0.002846317870231966, Val f1: 0.9993101954460144\n",
            "Val loss: 0.0027993042625624825, Val f1: 0.9994315505027771\n",
            "Val loss: 0.0028980590351339844, Val f1: 0.9992809295654297\n",
            "Val loss: 0.0028212860718162523, Val f1: 0.9992948174476624\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.0009112446394283324\n",
            "Train loss: 0.0010695442112462063\n",
            "Train loss: 0.001150967437396979\n",
            "Train loss: 0.001067028008817163\n",
            "Train loss: 0.001043918452258887\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.001522970721790833, Val f1: 0.9998354315757751\n",
            "Val loss: 0.0015290742191219969, Val f1: 0.9998372793197632\n",
            "Val loss: 0.0015188312008311705, Val f1: 0.9998645782470703\n",
            "Val loss: 0.001563443723716773, Val f1: 0.9998504519462585\n",
            "Val loss: 0.0015290125826972405, Val f1: 0.9998692274093628\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.002778795547783375, Val f1: 0.9991436004638672\n",
            "Val loss: 0.00279395859171119, Val f1: 0.9992592930793762\n",
            "Val loss: 0.0027199453303452443, Val f1: 0.9993976354598999\n",
            "Val loss: 0.002822015392464689, Val f1: 0.9992554783821106\n",
            "Val loss: 0.002728906057826761, Val f1: 0.999274492263794\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.0010567823799127447\n",
            "Train loss: 0.0010963664689500417\n",
            "Train loss: 0.0009700444526970387\n",
            "Train loss: 0.0009087329844727979\n",
            "Train loss: 0.0008678183930792979\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00086633358400182, Val f1: 0.9999435544013977\n",
            "Val loss: 0.0008081437142599108, Val f1: 0.9999717473983765\n",
            "Val loss: 0.0008612307409445445, Val f1: 0.999944806098938\n",
            "Val loss: 0.0008786042441248095, Val f1: 0.9999444484710693\n",
            "Val loss: 0.0008962760403353188, Val f1: 0.9999440312385559\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001877309023661332, Val f1: 0.9996840357780457\n",
            "Val loss: 0.001897600201320731, Val f1: 0.999736487865448\n",
            "Val loss: 0.00190256872964609, Val f1: 0.9996805191040039\n",
            "Val loss: 0.0019410159099303808, Val f1: 0.9996275305747986\n",
            "Val loss: 0.0018792389607470896, Val f1: 0.9996371865272522\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.0007567311204703791\n",
            "Train loss: 0.0007913554224484999\n",
            "Train loss: 0.0007996467441054328\n",
            "Train loss: 0.0008103109073999803\n",
            "Train loss: 0.0007910424571517589\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0008252028999517539, Val f1: 0.9999445080757141\n",
            "Val loss: 0.0008294355930827026, Val f1: 0.9999583959579468\n",
            "Val loss: 0.0008145401786480631, Val f1: 0.9999537467956543\n",
            "Val loss: 0.0007715191663010046, Val f1: 0.9999653697013855\n",
            "Val loss: 0.0008138150656928441, Val f1: 0.9999614357948303\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0017558414644251268, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0017841877286425894, Val f1: 0.999736487865448\n",
            "Val loss: 0.001784206311977296, Val f1: 0.9997157454490662\n",
            "Val loss: 0.0018254082064635844, Val f1: 0.9996540546417236\n",
            "Val loss: 0.00174729958590534, Val f1: 0.9996584057807922\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.0007152297392687095\n",
            "Train loss: 0.0007564425763640819\n",
            "Train loss: 0.0007443025522488391\n",
            "Train loss: 0.0007129987335896918\n",
            "Train loss: 0.0006924527133482376\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0007078797024275575, Val f1: 0.9999722242355347\n",
            "Val loss: 0.000712618071702309, Val f1: 0.999957799911499\n",
            "Val loss: 0.0007525905043751533, Val f1: 0.9999627470970154\n",
            "Val loss: 0.0007760092135868035, Val f1: 0.9999581575393677\n",
            "Val loss: 0.000801932278388579, Val f1: 0.9999557137489319\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001767114603555658, Val f1: 0.9995776414871216\n",
            "Val loss: 0.0017836293247657726, Val f1: 0.9995803833007812\n",
            "Val loss: 0.0017820329791262608, Val f1: 0.9996116757392883\n",
            "Val loss: 0.0018375251278889158, Val f1: 0.9995759129524231\n",
            "Val loss: 0.001767624029242951, Val f1: 0.9995959401130676\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.0005426549598009192\n",
            "Train loss: 0.0005677812144442994\n",
            "Train loss: 0.0006357198793141704\n",
            "Train loss: 0.0006139146748215093\n",
            "Train loss: 0.0006113957824917244\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00047439814453744994, Val f1: 0.9999719858169556\n",
            "Val loss: 0.0004681710318046888, Val f1: 0.9999860525131226\n",
            "Val loss: 0.000478399209705891, Val f1: 0.9999629259109497\n",
            "Val loss: 0.00047516677503673624, Val f1: 0.9999581575393677\n",
            "Val loss: 0.00046884718691996696, Val f1: 0.9999610185623169\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001241469112250747, Val f1: 0.9996908903121948\n",
            "Val loss: 0.0012614734037520571, Val f1: 0.9997931718826294\n",
            "Val loss: 0.0013205531760360355, Val f1: 0.9997535943984985\n",
            "Val loss: 0.0013043030541868778, Val f1: 0.9997355341911316\n",
            "Val loss: 0.0012637071887082938, Val f1: 0.9997445940971375\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.0005271414997488526\n",
            "Train loss: 0.0004697512044263671\n",
            "Train loss: 0.0005765213002589354\n",
            "Train loss: 0.0005910403900218496\n",
            "Train loss: 0.0006144765117122525\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.000398794946210858, Val f1: 0.9999186992645264\n",
            "Val loss: 0.00036379075359686147, Val f1: 0.9999593496322632\n",
            "Val loss: 0.00035927754915541127, Val f1: 0.9999637603759766\n",
            "Val loss: 0.0003843813518220226, Val f1: 0.9999521374702454\n",
            "Val loss: 0.000382513159087726, Val f1: 0.9999561309814453\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0010867994537370072, Val f1: 0.9996908903121948\n",
            "Val loss: 0.0011098981647390043, Val f1: 0.9997931718826294\n",
            "Val loss: 0.0011865060196551323, Val f1: 0.9997169375419617\n",
            "Val loss: 0.0011289209198568845, Val f1: 0.9997347593307495\n",
            "Val loss: 0.0011129250439504783, Val f1: 0.9997439980506897\n",
            "\n",
            "starting Epoch 25\n",
            "Training...\n",
            "Train loss: 0.0004756799017611359\n",
            "Train loss: 0.0005293991418771579\n",
            "Train loss: 0.0005448855501266995\n",
            "Train loss: 0.00050845901766609\n",
            "Train loss: 0.0004994316046941094\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00045667468886157234, Val f1: 1.0\n",
            "Val loss: 0.0004601373515989897, Val f1: 1.0\n",
            "Val loss: 0.00043961496627335214, Val f1: 1.0\n",
            "Val loss: 0.0004470091721616752, Val f1: 0.9999858140945435\n",
            "Val loss: 0.00045486127201002093, Val f1: 0.9999828338623047\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0012409076023484683, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0012593991656710084, Val f1: 0.9997896552085876\n",
            "Val loss: 0.0012947877241660738, Val f1: 0.9997864961624146\n",
            "Val loss: 0.0013025459314424854, Val f1: 0.999732255935669\n",
            "Val loss: 0.0012508893527814911, Val f1: 0.9997419714927673\n",
            "\n",
            "starting Epoch 26\n",
            "Training...\n",
            "Train loss: 0.0005085923567613853\n",
            "Train loss: 0.0004543780324248863\n",
            "Train loss: 0.0004098066367164609\n",
            "Train loss: 0.0004378074744766179\n",
            "Train loss: 0.00042706910990610985\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0002960536683011534, Val f1: 1.0\n",
            "Val loss: 0.0003548299949865655, Val f1: 0.9999861717224121\n",
            "Val loss: 0.00036583482467990726, Val f1: 0.9999908208847046\n",
            "Val loss: 0.0003810920561331191, Val f1: 0.9999861717224121\n",
            "Val loss: 0.0003911830283634897, Val f1: 0.9999832510948181\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001134187335588245, Val f1: 0.9996840357780457\n",
            "Val loss: 0.001170439220408702, Val f1: 0.9997896552085876\n",
            "Val loss: 0.0012162419292575646, Val f1: 0.9997864961624146\n",
            "Val loss: 0.0012109729230158134, Val f1: 0.9997602105140686\n",
            "Val loss: 0.0011522986919670884, Val f1: 0.9997643828392029\n",
            "\n",
            "starting Epoch 27\n",
            "Training...\n",
            "Train loss: 0.00043372144802041084\n",
            "Train loss: 0.00042829087376178776\n",
            "Train loss: 0.00042137921672768977\n",
            "Train loss: 0.00042702419289396076\n",
            "Train loss: 0.00045746549406820643\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00034370529091185226, Val f1: 1.0\n",
            "Val loss: 0.00036814847044297494, Val f1: 0.9999860525131226\n",
            "Val loss: 0.0003643079143200469, Val f1: 0.9999907612800598\n",
            "Val loss: 0.0003684966229359686, Val f1: 0.999985933303833\n",
            "Val loss: 0.0003886944237664076, Val f1: 0.999983012676239\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001148024645064854, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0011658659285684633, Val f1: 0.9997896552085876\n",
            "Val loss: 0.0012088361556245083, Val f1: 0.9997512102127075\n",
            "Val loss: 0.001218739996450798, Val f1: 0.9997057318687439\n",
            "Val loss: 0.001162217635040482, Val f1: 0.9997208118438721\n",
            "\n",
            "starting Epoch 28\n",
            "Training...\n",
            "Train loss: 0.00034711250752609754\n",
            "Train loss: 0.0003699148695367122\n",
            "Train loss: 0.00036916530461028396\n",
            "Train loss: 0.00039149151120260027\n",
            "Train loss: 0.0003645894496418935\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.000229741312380481, Val f1: 1.0\n",
            "Val loss: 0.00026281870273773427, Val f1: 1.0\n",
            "Val loss: 0.00028781032437802335, Val f1: 0.999981701374054\n",
            "Val loss: 0.0002908909242770668, Val f1: 0.9999862909317017\n",
            "Val loss: 0.00029922980727860706, Val f1: 0.9999834895133972\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001007570788109054, Val f1: 0.999792218208313\n",
            "Val loss: 0.0010317885850478585, Val f1: 0.9998438358306885\n",
            "Val loss: 0.0010998311449333818, Val f1: 0.9998226165771484\n",
            "Val loss: 0.0010904088141817031, Val f1: 0.9997873306274414\n",
            "Val loss: 0.0010390398160476859, Val f1: 0.9997860789299011\n",
            "\n",
            "starting Epoch 29\n",
            "Training...\n",
            "Train loss: 0.0002134821134469738\n",
            "Train loss: 0.00026070842380119884\n",
            "Train loss: 0.00028879495526087424\n",
            "Train loss: 0.0003068159430508136\n",
            "Train loss: 0.00032205287541728464\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00019110364436138687, Val f1: 1.0\n",
            "Val loss: 0.0002039472340714253, Val f1: 1.0\n",
            "Val loss: 0.00021100872600938948, Val f1: 0.999991238117218\n",
            "Val loss: 0.00021600921666374364, Val f1: 0.999986469745636\n",
            "Val loss: 0.00021578825278473752, Val f1: 0.9999891519546509\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008644797538484758, Val f1: 0.9997923374176025\n",
            "Val loss: 0.0008685022427622849, Val f1: 0.9998438358306885\n",
            "Val loss: 0.0009580723875573043, Val f1: 0.9997873902320862\n",
            "Val loss: 0.0008910458018362988, Val f1: 0.999812662601471\n",
            "Val loss: 0.0008781726947442319, Val f1: 0.9998063445091248\n",
            "\n",
            "starting Epoch 30\n",
            "Training...\n",
            "Train loss: 0.0002756254664356155\n",
            "Train loss: 0.00032457043115365585\n",
            "Train loss: 0.00030595932351258983\n",
            "Train loss: 0.00029094385988303527\n",
            "Train loss: 0.0003131365877509649\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00023032343326901485, Val f1: 0.9999723434448242\n",
            "Val loss: 0.0002179016019778958, Val f1: 0.9999861717224121\n",
            "Val loss: 0.00020756030817643652, Val f1: 0.9999908208847046\n",
            "Val loss: 0.00020626920265515635, Val f1: 0.9999930262565613\n",
            "Val loss: 0.00021321195508270258, Val f1: 0.9999889135360718\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.000848904313493727, Val f1: 0.999792218208313\n",
            "Val loss: 0.0008641919484944083, Val f1: 0.9998438358306885\n",
            "Val loss: 0.000934114523395827, Val f1: 0.9998226165771484\n",
            "Val loss: 0.0009090132435732004, Val f1: 0.9997873306274414\n",
            "Val loss: 0.0008689985099610769, Val f1: 0.9997860789299011\n",
            "\n",
            "starting Epoch 31\n",
            "Training...\n",
            "Train loss: 0.00043197405798959415\n",
            "Train loss: 0.0003482719499256096\n",
            "Train loss: 0.0003307319349481813\n",
            "Train loss: 0.0003096285514142697\n",
            "Train loss: 0.0003061694964084641\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0001760619815156263, Val f1: 1.0\n",
            "Val loss: 0.00016997733868525497, Val f1: 1.0\n",
            "Val loss: 0.000175876108377928, Val f1: 1.0\n",
            "Val loss: 0.0001840579241875925, Val f1: 0.9999927878379822\n",
            "Val loss: 0.00018169963630498386, Val f1: 0.9999889135360718\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008196532386743153, Val f1: 0.999792218208313\n",
            "Val loss: 0.000806583191054718, Val f1: 0.9998438358306885\n",
            "Val loss: 0.0008842929221120559, Val f1: 0.9998226165771484\n",
            "Val loss: 0.0008506127633154392, Val f1: 0.9998391270637512\n",
            "Val loss: 0.0008228788242882325, Val f1: 0.99982750415802\n",
            "\n",
            "starting Epoch 32\n",
            "Training...\n",
            "Train loss: 0.00035534646844358317\n",
            "Train loss: 0.0003146311926164864\n",
            "Train loss: 0.00030592900750759457\n",
            "Train loss: 0.000288731348544908\n",
            "Train loss: 0.00027994630770990627\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0005437427100592426, Val f1: 0.9999434351921082\n",
            "Val loss: 0.0005970237236137369, Val f1: 0.9999427795410156\n",
            "Val loss: 0.0006043706887534687, Val f1: 0.9999438524246216\n",
            "Val loss: 0.0006035886222629675, Val f1: 0.9999505281448364\n",
            "Val loss: 0.000575446740715831, Val f1: 0.9999549388885498\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0016226151363096302, Val f1: 0.9996883869171143\n",
            "Val loss: 0.0016143003465711242, Val f1: 0.9995821714401245\n",
            "Val loss: 0.0015584391528844005, Val f1: 0.9996128678321838\n",
            "Val loss: 0.0016453090114130948, Val f1: 0.9995215535163879\n",
            "Val loss: 0.0015616360773694597, Val f1: 0.9995944499969482\n",
            "\n",
            "starting Epoch 33\n",
            "Training...\n",
            "Train loss: 0.0004017113655988526\n",
            "Train loss: 0.00034870228477562446\n",
            "Train loss: 0.00032071112897635125\n",
            "Train loss: 0.00030477055141610825\n",
            "Train loss: 0.0002933537696870709\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00016234344594912337, Val f1: 1.0\n",
            "Val loss: 0.0001717204716572139, Val f1: 1.0\n",
            "Val loss: 0.0001876975465122433, Val f1: 0.9999905824661255\n",
            "Val loss: 0.00018656693170799243, Val f1: 0.9999858140945435\n",
            "Val loss: 0.00018420660018039469, Val f1: 0.9999886751174927\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0007736138682553752, Val f1: 0.9998936057090759\n",
            "Val loss: 0.0007706414544372819, Val f1: 0.9998944401741028\n",
            "Val loss: 0.0008431148313253221, Val f1: 0.9998564124107361\n",
            "Val loss: 0.0008318950139154266, Val f1: 0.9998392462730408\n",
            "Val loss: 0.0007955692556505609, Val f1: 0.9998486042022705\n",
            "\n",
            "starting Epoch 34\n",
            "Training...\n",
            "Train loss: 0.00023204369499580936\n",
            "Train loss: 0.00023358158588442684\n",
            "Train loss: 0.0002201514254487674\n",
            "Train loss: 0.0002470745891124742\n",
            "Train loss: 0.0002475860574798259\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00019628039652681245, Val f1: 0.9999721050262451\n",
            "Val loss: 0.00020209159863173096, Val f1: 0.9999860525131226\n",
            "Val loss: 0.00019679723832052246, Val f1: 0.9999815821647644\n",
            "Val loss: 0.00018976800887945242, Val f1: 0.9999861717224121\n",
            "Val loss: 0.00018513589070477923, Val f1: 0.9999889135360718\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008123308135610488, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0007951522388288544, Val f1: 0.9997896552085876\n",
            "Val loss: 0.0008666657829760677, Val f1: 0.9997864961624146\n",
            "Val loss: 0.0008634996940802214, Val f1: 0.9997602105140686\n",
            "Val loss: 0.0008250831872121327, Val f1: 0.9997853636741638\n",
            "\n",
            "starting Epoch 35\n",
            "Training...\n",
            "Train loss: 0.000253584702711253\n",
            "Train loss: 0.0002438182059060117\n",
            "Train loss: 0.0002432133703786392\n",
            "Train loss: 0.00021577807861571533\n",
            "Train loss: 0.00020704321748261074\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00013955635742084787, Val f1: 0.9999722242355347\n",
            "Val loss: 0.00012713777344158317, Val f1: 0.9999860525131226\n",
            "Val loss: 0.00012844185375364585, Val f1: 0.9999907612800598\n",
            "Val loss: 0.00012572079332520453, Val f1: 0.9999930262565613\n",
            "Val loss: 0.00012190719249442087, Val f1: 0.9999943971633911\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006904524537579467, Val f1: 0.9998936057090759\n",
            "Val loss: 0.0006933897311682813, Val f1: 0.9998944401741028\n",
            "Val loss: 0.000788749304774683, Val f1: 0.9998564124107361\n",
            "Val loss: 0.0007383765726748001, Val f1: 0.9998392462730408\n",
            "Val loss: 0.0006921621435645244, Val f1: 0.9998276233673096\n",
            "\n",
            "starting Epoch 36\n",
            "Training...\n",
            "Train loss: 0.00017882186356083757\n",
            "Train loss: 0.00026226337722619065\n",
            "Train loss: 0.0002799285569857429\n",
            "Train loss: 0.00024095263738023017\n",
            "Train loss: 0.0002164947217014352\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00013199549263975184, Val f1: 1.0\n",
            "Val loss: 0.00012799215214077516, Val f1: 1.0\n",
            "Val loss: 0.00012326365017326592, Val f1: 1.0\n",
            "Val loss: 0.0001381133345993086, Val f1: 0.9999862909317017\n",
            "Val loss: 0.00013236152924946508, Val f1: 0.9999890327453613\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006766362300065035, Val f1: 0.9997855424880981\n",
            "Val loss: 0.0006822486826018172, Val f1: 0.9998404383659363\n",
            "Val loss: 0.0007711266958654893, Val f1: 0.999820351600647\n",
            "Val loss: 0.0007386467113974504, Val f1: 0.9997856616973877\n",
            "Val loss: 0.0006824113457696513, Val f1: 0.999805748462677\n",
            "\n",
            "starting Epoch 37\n",
            "Training...\n",
            "Train loss: 0.0001548299155339399\n",
            "Train loss: 0.0001366838164748125\n",
            "Train loss: 0.0001468000993121504\n",
            "Train loss: 0.00014092718784013413\n",
            "Train loss: 0.0001621740885457257\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0002631039702724333, Val f1: 1.0\n",
            "Val loss: 0.0003079295333528924, Val f1: 0.9999716877937317\n",
            "Val loss: 0.0002880506296129343, Val f1: 0.9999716281890869\n",
            "Val loss: 0.00027931815386961846, Val f1: 0.9999786615371704\n",
            "Val loss: 0.00027503762666518535, Val f1: 0.9999828934669495\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001077681978737625, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0010591641403152607, Val f1: 0.9996867775917053\n",
            "Val loss: 0.0010930465224966476, Val f1: 0.9997179508209229\n",
            "Val loss: 0.0011243082638732933, Val f1: 0.9997087121009827\n",
            "Val loss: 0.001037178250650565, Val f1: 0.999744176864624\n",
            "\n",
            "starting Epoch 38\n",
            "Training...\n",
            "Train loss: 0.00013536699142215573\n",
            "Train loss: 0.0001261343035626591\n",
            "Train loss: 0.00010973886291183243\n",
            "Train loss: 0.0001233454428724404\n",
            "Train loss: 0.000133952794567449\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00013314067088815917, Val f1: 1.0\n",
            "Val loss: 0.00018044359847928197, Val f1: 0.9999865889549255\n",
            "Val loss: 0.00017681905509172273, Val f1: 0.9999819397926331\n",
            "Val loss: 0.0001732910650649241, Val f1: 0.999986469745636\n",
            "Val loss: 0.0001665441703932759, Val f1: 0.9999891519546509\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008271519359873815, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0008022582195634539, Val f1: 0.9997896552085876\n",
            "Val loss: 0.0008807601121521589, Val f1: 0.9997864961624146\n",
            "Val loss: 0.0008798331655270886, Val f1: 0.9997602105140686\n",
            "Val loss: 0.0008126006115667729, Val f1: 0.9997853636741638\n",
            "\n",
            "starting Epoch 39\n",
            "Training...\n",
            "Train loss: 0.00012909862494520246\n",
            "Train loss: 0.0001298491945947587\n",
            "Train loss: 0.00012608162280576792\n",
            "Train loss: 0.0001321192835738267\n",
            "Train loss: 0.00013850167785546677\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00012075122966572442, Val f1: 1.0\n",
            "Val loss: 0.00011710668454075598, Val f1: 0.9999862909317017\n",
            "Val loss: 0.00012622859467228408, Val f1: 0.9999908804893494\n",
            "Val loss: 0.00011909265559942078, Val f1: 0.9999931454658508\n",
            "Val loss: 0.00012492639576521468, Val f1: 0.9999890327453613\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0007110642472980544, Val f1: 0.9996840357780457\n",
            "Val loss: 0.0006991654570608969, Val f1: 0.9997896552085876\n",
            "Val loss: 0.0007975787162582425, Val f1: 0.9997864961624146\n",
            "Val loss: 0.0007833103259650266, Val f1: 0.9997602105140686\n",
            "Val loss: 0.0007267006608243617, Val f1: 0.9997853636741638\n",
            "\n",
            "starting Epoch 40\n",
            "Training...\n",
            "Train loss: 0.00011980697366068074\n",
            "Train loss: 0.0001480769216608938\n",
            "Train loss: 0.00012555520853701246\n",
            "Train loss: 0.00013225996107004383\n",
            "Train loss: 0.00013780131952704063\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 9.376860949227454e-05, Val f1: 1.0\n",
            "Val loss: 0.00011022661840667882, Val f1: 1.0\n",
            "Val loss: 0.00012413850831979376, Val f1: 0.999991238117218\n",
            "Val loss: 0.00011339494911745922, Val f1: 0.9999934434890747\n",
            "Val loss: 0.00010907346268400683, Val f1: 0.9999947547912598\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006639279664442358, Val f1: 0.9998936057090759\n",
            "Val loss: 0.0006666703961349817, Val f1: 0.9998944401741028\n",
            "Val loss: 0.0007644236231701345, Val f1: 0.9998564124107361\n",
            "Val loss: 0.0007469177548450211, Val f1: 0.9998125433921814\n",
            "Val loss: 0.0006906181240790627, Val f1: 0.9998272657394409\n",
            "\n",
            "starting Epoch 41\n",
            "Training...\n",
            "Train loss: 0.00010201106763685987\n",
            "Train loss: 0.00014174788029777118\n",
            "Train loss: 0.00013265041516639204\n",
            "Train loss: 0.00012082955726587638\n",
            "Train loss: 0.00011563215577812765\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 8.308728450044458e-05, Val f1: 1.0\n",
            "Val loss: 8.35106534265963e-05, Val f1: 1.0\n",
            "Val loss: 7.879488969462858e-05, Val f1: 1.0\n",
            "Val loss: 8.310712749204997e-05, Val f1: 1.0\n",
            "Val loss: 8.480923230568545e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006191102090977236, Val f1: 0.9998936057090759\n",
            "Val loss: 0.0006261062529423119, Val f1: 0.9998944401741028\n",
            "Val loss: 0.0007294557936903504, Val f1: 0.9998564124107361\n",
            "Val loss: 0.0006801739493514308, Val f1: 0.9998656511306763\n",
            "Val loss: 0.0006255819898797199, Val f1: 0.9998487234115601\n",
            "\n",
            "starting Epoch 42\n",
            "Training...\n",
            "Train loss: 8.508791673063699e-05\n",
            "Train loss: 8.677971943273275e-05\n",
            "Train loss: 0.00010657163788675375\n",
            "Train loss: 0.00010867197488551028\n",
            "Train loss: 0.00010116479174549957\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 6.222019011537278e-05, Val f1: 1.0\n",
            "Val loss: 7.360284094569839e-05, Val f1: 1.0\n",
            "Val loss: 7.152100669849287e-05, Val f1: 1.0\n",
            "Val loss: 7.2940566899758e-05, Val f1: 1.0\n",
            "Val loss: 7.366786466133947e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0005773346743404141, Val f1: 0.9998936057090759\n",
            "Val loss: 0.0005653982613390932, Val f1: 0.9998944401741028\n",
            "Val loss: 0.0006696423054866804, Val f1: 0.9998564124107361\n",
            "Val loss: 0.0006329870284389472, Val f1: 0.9998923540115356\n",
            "Val loss: 0.0005932943976303148, Val f1: 0.9998911023139954\n",
            "\n",
            "starting Epoch 43\n",
            "Training...\n",
            "Train loss: 9.788085977301567e-05\n",
            "Train loss: 7.978897500444353e-05\n",
            "Train loss: 9.260518663289813e-05\n",
            "Train loss: 0.0001060821896122174\n",
            "Train loss: 0.0001109245522249174\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 5.70100807505826e-05, Val f1: 1.0\n",
            "Val loss: 5.5647903978492e-05, Val f1: 1.0\n",
            "Val loss: 5.23908678760996e-05, Val f1: 1.0\n",
            "Val loss: 5.3828161006614596e-05, Val f1: 1.0\n",
            "Val loss: 5.347553783524615e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0005702417543539519, Val f1: 0.9997923374176025\n",
            "Val loss: 0.0005467004227992018, Val f1: 0.9998438358306885\n",
            "Val loss: 0.0006545545569409754, Val f1: 0.9997873902320862\n",
            "Val loss: 0.0005753576432552007, Val f1: 0.9998405575752258\n",
            "Val loss: 0.0005394637479589114, Val f1: 0.9998286366462708\n",
            "\n",
            "starting Epoch 44\n",
            "Training...\n",
            "Train loss: 7.831126256080876e-05\n",
            "Train loss: 9.050320730758748e-05\n",
            "Train loss: 0.00010178317425882033\n",
            "Train loss: 9.424099673326834e-05\n",
            "Train loss: 9.048399546892532e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 7.278988079113852e-05, Val f1: 0.9999716877937317\n",
            "Val loss: 8.162224240939914e-05, Val f1: 0.9999722242355347\n",
            "Val loss: 7.476711686779578e-05, Val f1: 0.9999815225601196\n",
            "Val loss: 7.240110115032751e-05, Val f1: 0.9999860525131226\n",
            "Val loss: 7.024056452792138e-05, Val f1: 0.999988853931427\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0005667043874887491, Val f1: 0.9997855424880981\n",
            "Val loss: 0.0005581645192352072, Val f1: 0.9998404383659363\n",
            "Val loss: 0.0006758033408981713, Val f1: 0.999820351600647\n",
            "Val loss: 0.0006460613506836429, Val f1: 0.9998385310173035\n",
            "Val loss: 0.0005886631618422043, Val f1: 0.9998480081558228\n",
            "\n",
            "starting Epoch 45\n",
            "Training...\n",
            "Train loss: 6.620530795251918e-05\n",
            "Train loss: 6.413819358255881e-05\n",
            "Train loss: 6.536539124984605e-05\n",
            "Train loss: 9.639468019356303e-05\n",
            "Train loss: 9.790968144183612e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 5.3651897802150676e-05, Val f1: 1.0\n",
            "Val loss: 4.5231819201165177e-05, Val f1: 1.0\n",
            "Val loss: 4.018704466510653e-05, Val f1: 1.0\n",
            "Val loss: 4.573605152212882e-05, Val f1: 1.0\n",
            "Val loss: 4.5735349018026945e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006066718821481724, Val f1: 0.9997923374176025\n",
            "Val loss: 0.000553177583646579, Val f1: 0.9998438358306885\n",
            "Val loss: 0.0006819803383458984, Val f1: 0.9997873902320862\n",
            "Val loss: 0.0005855483332703039, Val f1: 0.9998405575752258\n",
            "Val loss: 0.0005505369311221229, Val f1: 0.9998286366462708\n",
            "\n",
            "starting Epoch 46\n",
            "Training...\n",
            "Train loss: 0.0005041087548826389\n",
            "Train loss: 0.0003094542545308546\n",
            "Train loss: 0.00023635840421515502\n",
            "Train loss: 0.00018979902721249735\n",
            "Train loss: 0.00016780723602226186\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00013305453586716405, Val f1: 0.9999727606773376\n",
            "Val loss: 0.0001302490779380605, Val f1: 0.9999864101409912\n",
            "Val loss: 0.00011961548229703026, Val f1: 0.9999909400939941\n",
            "Val loss: 0.00012101966028369914, Val f1: 0.9999864101409912\n",
            "Val loss: 0.0001181772544701484, Val f1: 0.9999890923500061\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0007253357907757163, Val f1: 0.9997855424880981\n",
            "Val loss: 0.0006837081208585813, Val f1: 0.9998404383659363\n",
            "Val loss: 0.0007808459752898974, Val f1: 0.999820351600647\n",
            "Val loss: 0.0008028680389947516, Val f1: 0.9997856616973877\n",
            "Val loss: 0.0007249781868368801, Val f1: 0.999805748462677\n",
            "\n",
            "starting Epoch 47\n",
            "Training...\n",
            "Train loss: 0.00010835400049212954\n",
            "Train loss: 8.908825046611517e-05\n",
            "Train loss: 9.143017189108788e-05\n",
            "Train loss: 8.779566306397361e-05\n",
            "Train loss: 8.645715721546107e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00011058131884575623, Val f1: 0.999971866607666\n",
            "Val loss: 8.716489237485803e-05, Val f1: 0.999985933303833\n",
            "Val loss: 8.909967442117964e-05, Val f1: 0.9999906420707703\n",
            "Val loss: 8.528236604174058e-05, Val f1: 0.9999930262565613\n",
            "Val loss: 8.702391218581137e-05, Val f1: 0.9999943971633911\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0005909524423057317, Val f1: 0.9997855424880981\n",
            "Val loss: 0.0005577302102513689, Val f1: 0.9998404383659363\n",
            "Val loss: 0.0006879726773789756, Val f1: 0.999820351600647\n",
            "Val loss: 0.000674796154372517, Val f1: 0.9997856616973877\n",
            "Val loss: 0.0006050707440105422, Val f1: 0.999805748462677\n",
            "\n",
            "starting Epoch 48\n",
            "Training...\n",
            "Train loss: 6.126583957666298e-05\n",
            "Train loss: 5.219930349734828e-05\n",
            "Train loss: 5.305158748299055e-05\n",
            "Train loss: 4.662285565310802e-05\n",
            "Train loss: 8.412224374426711e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 4.163700700051517e-05, Val f1: 1.0\n",
            "Val loss: 3.8398522298978475e-05, Val f1: 1.0\n",
            "Val loss: 3.691779229051289e-05, Val f1: 1.0\n",
            "Val loss: 3.465944521069884e-05, Val f1: 1.0\n",
            "Val loss: 3.448417600728655e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0004697492408417424, Val f1: 0.9998936057090759\n",
            "Val loss: 0.0004206540266346161, Val f1: 0.9998944401741028\n",
            "Val loss: 0.0005540173252585293, Val f1: 0.9998564124107361\n",
            "Val loss: 0.0004889068110666509, Val f1: 0.9998923540115356\n",
            "Val loss: 0.00044804916259535174, Val f1: 0.9998911023139954\n",
            "\n",
            "starting Epoch 49\n",
            "Training...\n",
            "Train loss: 6.846811606270161e-05\n",
            "Train loss: 6.638360524188361e-05\n",
            "Train loss: 0.00010798125235201691\n",
            "Train loss: 9.382356331246098e-05\n",
            "Train loss: 8.593664895410516e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 5.323659921227123e-05, Val f1: 1.0\n",
            "Val loss: 4.727429140984896e-05, Val f1: 1.0\n",
            "Val loss: 4.517105351710814e-05, Val f1: 1.0\n",
            "Val loss: 4.543708409008624e-05, Val f1: 1.0\n",
            "Val loss: 4.276469823577957e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0005128284103395547, Val f1: 0.9997855424880981\n",
            "Val loss: 0.0004727034448579717, Val f1: 0.9998404383659363\n",
            "Val loss: 0.0006093574949638289, Val f1: 0.999820351600647\n",
            "Val loss: 0.0005634104225767563, Val f1: 0.9998385310173035\n",
            "Val loss: 0.0005094979860182826, Val f1: 0.9998480081558228\n"
          ]
        }
      ],
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(50):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator2, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator2, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator2, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Улучшим, уберём дропаут. За меньшее количество эпох результат получится лучше."
      ],
      "metadata": {
        "id": "tHd9gIhGZmIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 модель"
      ],
      "metadata": {
        "id": "drceV5hSZWoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "mMyXDXW8_qkl"
      },
      "outputs": [],
      "source": [
        "class CNN_2(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size_w, embedding_dim_w, vocab_size_s, embedding_dim_s):\n",
        "        super().__init__()\n",
        "        self.embedding_w = nn.Embedding(vocab_size_w, embedding_dim_w)\n",
        "        self.embedding_s = nn.Embedding(vocab_size_s, embedding_dim_s)\n",
        "        self.hid_for_senten = nn.Linear(in_features=embedding_dim_w, out_features=embedding_dim_w//2)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim_s, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim_s, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180+embedding_dim_w//2, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, padded_ids_w, padded_ids_s):\n",
        "        words = padded_ids_w\n",
        "        symbols = padded_ids_s\n",
        "        \n",
        "        #batch_size x seq_len\n",
        "        embedded_w = self.embedding_w(words)\n",
        "        embedded_sentence = torch.mean(embedded_w, dim=1)\n",
        "        vec_X = self.hid_for_senten(embedded_sentence)\n",
        "        \n",
        "        #batch_size x seq_len\n",
        "        embedded_s = self.embedding_s(symbols)\n",
        "        \n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded_s = embedded_s.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.pooling(self.relu(self.bigrams(embedded_s)))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.pooling(self.relu(self.trigrams(embedded_s)))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        # batch_size x filter_count2\n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((pooling1, pooling2, vec_X), 1)\n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_ifTbd6b_qkk"
      },
      "outputs": [],
      "source": [
        "model = CNN_2(len(word2id), 186, len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(30):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator2, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator2, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator2, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNAXfWxraotv",
        "outputId": "92f8ba11-3f0d-4f62-9e49-376a20f7b98f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.6432758058820452\n",
            "Train loss: 0.607628516639982\n",
            "Train loss: 0.5574081457796551\n",
            "Train loss: 0.4997933302606855\n",
            "Train loss: 0.4437719122001103\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.17773547513144358, Val f1: 0.9639342427253723\n",
            "Val loss: 0.17588233245270593, Val f1: 0.9646584987640381\n",
            "Val loss: 0.17557115157445272, Val f1: 0.9647602438926697\n",
            "Val loss: 0.17531021301235472, Val f1: 0.9652224779129028\n",
            "Val loss: 0.17499568598611015, Val f1: 0.9654300212860107\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.17225799295637342, Val f1: 0.9684064984321594\n",
            "Val loss: 0.17355242123206457, Val f1: 0.9679866433143616\n",
            "Val loss: 0.1741201270509649, Val f1: 0.9664027690887451\n",
            "Val loss: 0.17567125086983046, Val f1: 0.9653493165969849\n",
            "Val loss: 0.17623135613070595, Val f1: 0.9653328061103821\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.15376799958092827\n",
            "Train loss: 0.1365033404103347\n",
            "Train loss: 0.12360749883311135\n",
            "Train loss: 0.1143392311675208\n",
            "Train loss: 0.10691445380449295\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.06950839598264014, Val f1: 0.9764665961265564\n",
            "Val loss: 0.07022652035312993, Val f1: 0.9760774970054626\n",
            "Val loss: 0.0706861840472335, Val f1: 0.9759766459465027\n",
            "Val loss: 0.0701153386650341, Val f1: 0.9762473106384277\n",
            "Val loss: 0.07028345103774752, Val f1: 0.9760761260986328\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.06766905014713605, Val f1: 0.979321300983429\n",
            "Val loss: 0.06815913878381252, Val f1: 0.9788309931755066\n",
            "Val loss: 0.0684370703443333, Val f1: 0.9781062006950378\n",
            "Val loss: 0.07032598979357216, Val f1: 0.9771886467933655\n",
            "Val loss: 0.07100552668174108, Val f1: 0.9766421318054199\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.0661219739488193\n",
            "Train loss: 0.06289331008281027\n",
            "Train loss: 0.05900725626519748\n",
            "Train loss: 0.0551120700314641\n",
            "Train loss: 0.051607828864029474\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.033246074563690595, Val f1: 0.9892427921295166\n",
            "Val loss: 0.03350266888737678, Val f1: 0.988847553730011\n",
            "Val loss: 0.03382309199798675, Val f1: 0.9886565208435059\n",
            "Val loss: 0.03417666082137397, Val f1: 0.9883308410644531\n",
            "Val loss: 0.03408979321164744, Val f1: 0.9884041547775269\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.032343429409795336, Val f1: 0.9906338453292847\n",
            "Val loss: 0.032805399141377874, Val f1: 0.9894266128540039\n",
            "Val loss: 0.03323760242373855, Val f1: 0.9887852668762207\n",
            "Val loss: 0.03465785344855653, Val f1: 0.9878320693969727\n",
            "Val loss: 0.03518693380885654, Val f1: 0.9872737526893616\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.031171229747789245\n",
            "Train loss: 0.028936722369066307\n",
            "Train loss: 0.027331964086209026\n",
            "Train loss: 0.025486782698759012\n",
            "Train loss: 0.02366512445466859\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.015005629429859774, Val f1: 0.9967180490493774\n",
            "Val loss: 0.014999791833439044, Val f1: 0.9966681599617004\n",
            "Val loss: 0.014798083156347275, Val f1: 0.9967849254608154\n",
            "Val loss: 0.01502345947415701, Val f1: 0.9967047572135925\n",
            "Val loss: 0.01502587397715875, Val f1: 0.9966890811920166\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.014631417269508043, Val f1: 0.9970143437385559\n",
            "Val loss: 0.014700680402004056, Val f1: 0.9969670176506042\n",
            "Val loss: 0.01492388318810198, Val f1: 0.9969276785850525\n",
            "Val loss: 0.01567061095395022, Val f1: 0.9963013529777527\n",
            "Val loss: 0.015958038344979285, Val f1: 0.9961218237876892\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.01346730294504336\n",
            "Train loss: 0.012823931021349771\n",
            "Train loss: 0.01218601075843686\n",
            "Train loss: 0.011520399090035686\n",
            "Train loss: 0.010837203759167876\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.007688423246145248, Val f1: 0.9990020990371704\n",
            "Val loss: 0.007372158799054367, Val f1: 0.999102771282196\n",
            "Val loss: 0.007387561501846427, Val f1: 0.9990780353546143\n",
            "Val loss: 0.0074239724348964435, Val f1: 0.9990682601928711\n",
            "Val loss: 0.007486502746386187, Val f1: 0.999038577079773\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.007784724080314239, Val f1: 0.9983035326004028\n",
            "Val loss: 0.007697619213205244, Val f1: 0.998462975025177\n",
            "Val loss: 0.00772634365906318, Val f1: 0.9986128211021423\n",
            "Val loss: 0.008074904274609353, Val f1: 0.9985021948814392\n",
            "Val loss: 0.008236405170626111, Val f1: 0.9985446929931641\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.007192565061684165\n",
            "Train loss: 0.0068359719256737405\n",
            "Train loss: 0.006544010199251629\n",
            "Train loss: 0.006214612141983318\n",
            "Train loss: 0.005917694216061916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.004380985349416733, Val f1: 0.9995928406715393\n",
            "Val loss: 0.004403161869517395, Val f1: 0.9996011853218079\n",
            "Val loss: 0.0045765978772015796, Val f1: 0.9995168447494507\n",
            "Val loss: 0.004605647608903902, Val f1: 0.9994789361953735\n",
            "Val loss: 0.0045345869367676124, Val f1: 0.9995158314704895\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0050433192195163835, Val f1: 0.9991433024406433\n",
            "Val loss: 0.0049766777859379845, Val f1: 0.9990985989570618\n",
            "Val loss: 0.004915298643970379, Val f1: 0.999180793762207\n",
            "Val loss: 0.005119115211224805, Val f1: 0.9991438984870911\n",
            "Val loss: 0.005228095744839973, Val f1: 0.9991655349731445\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.004305331748245018\n",
            "Train loss: 0.0042400461022875134\n",
            "Train loss: 0.004098720278679615\n",
            "Train loss: 0.003903962880472786\n",
            "Train loss: 0.0037358781435926046\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00283994306477585, Val f1: 0.9997996687889099\n",
            "Val loss: 0.0028726396500132977, Val f1: 0.9997750520706177\n",
            "Val loss: 0.002948872378051636, Val f1: 0.9997572302818298\n",
            "Val loss: 0.003021126179789592, Val f1: 0.9997354745864868\n",
            "Val loss: 0.003009756661818496, Val f1: 0.9997386932373047\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.003540797585931917, Val f1: 0.9993609189987183\n",
            "Val loss: 0.003504287716673894, Val f1: 0.9994184970855713\n",
            "Val loss: 0.0034030436848600707, Val f1: 0.9994667172431946\n",
            "Val loss: 0.00351225578924641, Val f1: 0.9994948506355286\n",
            "Val loss: 0.00356163426509334, Val f1: 0.999488115310669\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.002884669498806553\n",
            "Train loss: 0.002903066823325519\n",
            "Train loss: 0.002776390025835662\n",
            "Train loss: 0.002697923663072288\n",
            "Train loss: 0.0025938959093764423\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0023354896543813604, Val f1: 0.9996974468231201\n",
            "Val loss: 0.002203597181609699, Val f1: 0.9997807145118713\n",
            "Val loss: 0.002168827697945138, Val f1: 0.9998167753219604\n",
            "Val loss: 0.002153676502140505, Val f1: 0.999835193157196\n",
            "Val loss: 0.002160414693477963, Val f1: 0.9998188018798828\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0026384731366609535, Val f1: 0.9994627833366394\n",
            "Val loss: 0.002644876716658473, Val f1: 0.9994694590568542\n",
            "Val loss: 0.0025280015155052147, Val f1: 0.9995006918907166\n",
            "Val loss: 0.0025841512283982914, Val f1: 0.99957275390625\n",
            "Val loss: 0.0025921385787013506, Val f1: 0.9995941519737244\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.002063422846341772\n",
            "Train loss: 0.002090309202737574\n",
            "Train loss: 0.002013574818903137\n",
            "Train loss: 0.0019406235444226435\n",
            "Train loss: 0.001906615543245737\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0016385016863101295, Val f1: 0.9998356699943542\n",
            "Val loss: 0.0017140756660540188, Val f1: 0.9997549057006836\n",
            "Val loss: 0.0016715140281511204, Val f1: 0.9998005628585815\n",
            "Val loss: 0.0016470509843202307, Val f1: 0.9998225569725037\n",
            "Val loss: 0.0016188202129809985, Val f1: 0.9998412132263184\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0021691667433414194, Val f1: 0.9994627833366394\n",
            "Val loss: 0.002178983775795334, Val f1: 0.9994694590568542\n",
            "Val loss: 0.00204934693519371, Val f1: 0.9995006918907166\n",
            "Val loss: 0.0020950607124379734, Val f1: 0.99957275390625\n",
            "Val loss: 0.0020954878444576427, Val f1: 0.9995941519737244\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.0015905696383145239\n",
            "Train loss: 0.0015599015385045537\n",
            "Train loss: 0.001535539927204982\n",
            "Train loss: 0.0014770036559119553\n",
            "Train loss: 0.0014396819079826985\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0011956703000968055, Val f1: 1.0\n",
            "Val loss: 0.001187937963654154, Val f1: 0.9999464750289917\n",
            "Val loss: 0.001201405203236001, Val f1: 0.9999459981918335\n",
            "Val loss: 0.0012362174269843049, Val f1: 0.9999111890792847\n",
            "Val loss: 0.0012430353522566813, Val f1: 0.999900758266449\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0017747604740887052, Val f1: 0.9994627833366394\n",
            "Val loss: 0.0017944402869842532, Val f1: 0.9994694590568542\n",
            "Val loss: 0.0016553130656204841, Val f1: 0.9996094703674316\n",
            "Val loss: 0.0016775416216761288, Val f1: 0.9996543526649475\n",
            "Val loss: 0.0016603955675640867, Val f1: 0.9996811747550964\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.0011694077834753054\n",
            "Train loss: 0.001274273467216907\n",
            "Train loss: 0.0012304245826921293\n",
            "Train loss: 0.001177141274508488\n",
            "Train loss: 0.0011371002308026487\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.000873397896066308, Val f1: 0.9999721050262451\n",
            "Val loss: 0.0009090587180773062, Val f1: 0.9999582767486572\n",
            "Val loss: 0.0009101682819337363, Val f1: 0.9999722242355347\n",
            "Val loss: 0.0009545116992999932, Val f1: 0.9999523758888245\n",
            "Val loss: 0.0009809243293212994, Val f1: 0.9999347925186157\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0014973659094216095, Val f1: 0.9996787309646606\n",
            "Val loss: 0.0015268746550039698, Val f1: 0.9995773434638977\n",
            "Val loss: 0.001381319389178383, Val f1: 0.9996814131736755\n",
            "Val loss: 0.0013882920595480958, Val f1: 0.9997082948684692\n",
            "Val loss: 0.0013600370406897531, Val f1: 0.9997453689575195\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.0010772151165708368\n",
            "Train loss: 0.0009869142680794798\n",
            "Train loss: 0.0009426022813256297\n",
            "Train loss: 0.0009094105380687065\n",
            "Train loss: 0.0009045793313998729\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0008199090013347034, Val f1: 0.9999154210090637\n",
            "Val loss: 0.0008032000315974333, Val f1: 0.9999449849128723\n",
            "Val loss: 0.0008120510000957264, Val f1: 0.9999274015426636\n",
            "Val loss: 0.0008405887837787824, Val f1: 0.9999182820320129\n",
            "Val loss: 0.0008234472719154187, Val f1: 0.9999291896820068\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0013836123075129257, Val f1: 0.9995661973953247\n",
            "Val loss: 0.0013956116939273973, Val f1: 0.9995726346969604\n",
            "Val loss: 0.0012457245611585677, Val f1: 0.9996783137321472\n",
            "Val loss: 0.0012508988454808584, Val f1: 0.999705970287323\n",
            "Val loss: 0.0012207869207486511, Val f1: 0.9997437596321106\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.0006718322435127838\n",
            "Train loss: 0.0007103038086955036\n",
            "Train loss: 0.0007349693934277942\n",
            "Train loss: 0.0007580734068012264\n",
            "Train loss: 0.0007380284332404179\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0007268775370903313, Val f1: 0.9999178051948547\n",
            "Val loss: 0.0006691622204795879, Val f1: 0.9999589323997498\n",
            "Val loss: 0.0006654693580154951, Val f1: 0.9999541640281677\n",
            "Val loss: 0.0006637631652535804, Val f1: 0.9999655485153198\n",
            "Val loss: 0.0006509678703566481, Val f1: 0.9999724626541138\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0011332336030641778, Val f1: 0.9995661973953247\n",
            "Val loss: 0.001169881966133188, Val f1: 0.9996793866157532\n",
            "Val loss: 0.0010280750047294768, Val f1: 0.9997494220733643\n",
            "Val loss: 0.0010202703795382856, Val f1: 0.9997592568397522\n",
            "Val loss: 0.0009846739412751049, Val f1: 0.9997861385345459\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.0007180439906993083\n",
            "Train loss: 0.0006561228643736935\n",
            "Train loss: 0.0006377594761683473\n",
            "Train loss: 0.0006240303182234388\n",
            "Train loss: 0.0006110067001178063\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0005183850057489638, Val f1: 1.0\n",
            "Val loss: 0.0004920941925125329, Val f1: 1.0\n",
            "Val loss: 0.0005041635208596875, Val f1: 0.9999908804893494\n",
            "Val loss: 0.0005417865147007562, Val f1: 0.9999796152114868\n",
            "Val loss: 0.0005368624598486349, Val f1: 0.9999780058860779\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0010283996428673465, Val f1: 0.9996741414070129\n",
            "Val loss: 0.001059594808288643, Val f1: 0.9997333884239197\n",
            "Val loss: 0.0009161147754639387, Val f1: 0.9997854828834534\n",
            "Val loss: 0.0009002504860594248, Val f1: 0.99981290102005\n",
            "Val loss: 0.0008607726084948, Val f1: 0.999850332736969\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.0004947957907071604\n",
            "Train loss: 0.000508974605223297\n",
            "Train loss: 0.0004927719944611281\n",
            "Train loss: 0.0005183297591949148\n",
            "Train loss: 0.0005088627285190991\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00049322834654179, Val f1: 0.9999728798866272\n",
            "Val loss: 0.0004812097318270909, Val f1: 0.999986469745636\n",
            "Val loss: 0.0004779400844459555, Val f1: 0.9999810457229614\n",
            "Val loss: 0.00047777814855050696, Val f1: 0.9999858140945435\n",
            "Val loss: 0.00048173461184238214, Val f1: 0.9999831318855286\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0009062268945854157, Val f1: 0.9997804760932922\n",
            "Val loss: 0.0009597428757438643, Val f1: 0.9997864961624146\n",
            "Val loss: 0.0008224627279021122, Val f1: 0.9998576641082764\n",
            "Val loss: 0.0007969277268987045, Val f1: 0.9998931884765625\n",
            "Val loss: 0.0007564396215861456, Val f1: 0.999893307685852\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.0004153206865469526\n",
            "Train loss: 0.000417309082695283\n",
            "Train loss: 0.000433611524030788\n",
            "Train loss: 0.0004346904542347017\n",
            "Train loss: 0.0004371894813708163\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00037757141025005175, Val f1: 1.0\n",
            "Val loss: 0.00036930222538233336, Val f1: 0.9999867081642151\n",
            "Val loss: 0.00036998716864867934, Val f1: 0.9999911785125732\n",
            "Val loss: 0.000387232189885773, Val f1: 0.9999864101409912\n",
            "Val loss: 0.0003846004472247192, Val f1: 0.9999890923500061\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008790064441402339, Val f1: 0.9996741414070129\n",
            "Val loss: 0.0008900861398109959, Val f1: 0.9997333884239197\n",
            "Val loss: 0.000749675021291262, Val f1: 0.9998222589492798\n",
            "Val loss: 0.0007223990032798611, Val f1: 0.9998667240142822\n",
            "Val loss: 0.0006808540622134589, Val f1: 0.9998933672904968\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.0004236508663910042\n",
            "Train loss: 0.00040932796858084787\n",
            "Train loss: 0.0003913308111978473\n",
            "Train loss: 0.0003826219997530903\n",
            "Train loss: 0.0003663393732443053\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0003146015681392912, Val f1: 1.0\n",
            "Val loss: 0.0003075996843108442, Val f1: 1.0\n",
            "Val loss: 0.0003257466619418535, Val f1: 0.9999812841415405\n",
            "Val loss: 0.00032323075824284127, Val f1: 0.999985933303833\n",
            "Val loss: 0.0003297929737683652, Val f1: 0.9999887347221375\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008308613161918604, Val f1: 0.9996741414070129\n",
            "Val loss: 0.0008331730649741883, Val f1: 0.9997333884239197\n",
            "Val loss: 0.0006922962304627247, Val f1: 0.9998222589492798\n",
            "Val loss: 0.0006605080169619112, Val f1: 0.9998667240142822\n",
            "Val loss: 0.0006172812065213091, Val f1: 0.9998933672904968\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.0003233123978134245\n",
            "Train loss: 0.00032151666237041355\n",
            "Train loss: 0.000337501546406808\n",
            "Train loss: 0.0003181650631761711\n",
            "Train loss: 0.00031787393639595913\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00034572613492075884, Val f1: 0.9999743700027466\n",
            "Val loss: 0.0003116219657905666, Val f1: 0.9999871253967285\n",
            "Val loss: 0.00030095939873717723, Val f1: 0.9999914765357971\n",
            "Val loss: 0.0002940830621290453, Val f1: 0.9999935626983643\n",
            "Val loss: 0.00029191743046145087, Val f1: 0.9999948740005493\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008785179703004865, Val f1: 0.9996741414070129\n",
            "Val loss: 0.0008325045103750502, Val f1: 0.9997333884239197\n",
            "Val loss: 0.0006845242357855939, Val f1: 0.9998222589492798\n",
            "Val loss: 0.0006504045729040323, Val f1: 0.9998404383659363\n",
            "Val loss: 0.0006061263560291586, Val f1: 0.9998723864555359\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.00027070526349624353\n",
            "Train loss: 0.0002498520407243632\n",
            "Train loss: 0.00026365583062648687\n",
            "Train loss: 0.0002574865922464856\n",
            "Train loss: 0.0002744983769870097\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0002772277287606682, Val f1: 1.0\n",
            "Val loss: 0.00026621832813751616, Val f1: 1.0\n",
            "Val loss: 0.0002487789549160793, Val f1: 1.0\n",
            "Val loss: 0.0002528373954971487, Val f1: 0.9999934434890747\n",
            "Val loss: 0.0002511389525274613, Val f1: 0.9999947547912598\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.000844035048632779, Val f1: 0.9996741414070129\n",
            "Val loss: 0.0007878205708240987, Val f1: 0.9997333884239197\n",
            "Val loss: 0.0006411047372104669, Val f1: 0.9998222589492798\n",
            "Val loss: 0.0006039844532400215, Val f1: 0.9998667240142822\n",
            "Val loss: 0.0005595166611278222, Val f1: 0.9998933672904968\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.00023499004289208512\n",
            "Train loss: 0.0002424495896515769\n",
            "Train loss: 0.0002487149152633113\n",
            "Train loss: 0.00024045258048447846\n",
            "Train loss: 0.00023440340849837022\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00022158913488965482, Val f1: 1.0\n",
            "Val loss: 0.00019889583220771912, Val f1: 1.0\n",
            "Val loss: 0.00020426800890293505, Val f1: 0.9999911785125732\n",
            "Val loss: 0.0002077661767543759, Val f1: 0.9999933838844299\n",
            "Val loss: 0.0002163347922448468, Val f1: 0.9999891519546509\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0007140701733684788, Val f1: 0.9998874664306641\n",
            "Val loss: 0.0007016974280607732, Val f1: 0.9998400211334229\n",
            "Val loss: 0.0005659942223617152, Val f1: 0.999893307685852\n",
            "Val loss: 0.000522641771668633, Val f1: 0.9999200105667114\n",
            "Val loss: 0.00047799308910422645, Val f1: 0.999936044216156\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.00019741537461024044\n",
            "Train loss: 0.00022679146111061397\n",
            "Train loss: 0.00021756192374076429\n",
            "Train loss: 0.00021558228628626758\n",
            "Train loss: 0.00020712173637418474\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0001936559057087704, Val f1: 0.9999721050262451\n",
            "Val loss: 0.0002023408467461455, Val f1: 0.9999860525131226\n",
            "Val loss: 0.00018889900131311845, Val f1: 0.9999907612800598\n",
            "Val loss: 0.00019040501062720847, Val f1: 0.999985933303833\n",
            "Val loss: 0.00018600459466272567, Val f1: 0.9999887347221375\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0007208049662747524, Val f1: 0.9998874664306641\n",
            "Val loss: 0.0006819332120358013, Val f1: 0.9998400211334229\n",
            "Val loss: 0.0005451636017015618, Val f1: 0.999893307685852\n",
            "Val loss: 0.0004999445969183904, Val f1: 0.9999200105667114\n",
            "Val loss: 0.00045435673819156365, Val f1: 0.999936044216156\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.00016382507331270192\n",
            "Train loss: 0.00017557178154155347\n",
            "Train loss: 0.00018419543110732256\n",
            "Train loss: 0.00018013362750934903\n",
            "Train loss: 0.00018336080987605132\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00018049912593726602, Val f1: 1.0\n",
            "Val loss: 0.00018332151188847742, Val f1: 0.999986469745636\n",
            "Val loss: 0.00017291311187797137, Val f1: 0.9999909996986389\n",
            "Val loss: 0.00016699973278134296, Val f1: 0.9999932646751404\n",
            "Val loss: 0.00016598656952347872, Val f1: 0.9999945759773254\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.000762880740997692, Val f1: 0.99978107213974\n",
            "Val loss: 0.0006856732802083974, Val f1: 0.9997867941856384\n",
            "Val loss: 0.0005446108579808087, Val f1: 0.9998579025268555\n",
            "Val loss: 0.0004971295551917541, Val f1: 0.9998934268951416\n",
            "Val loss: 0.00045082211290718986, Val f1: 0.9999147653579712\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.00015816198636977268\n",
            "Train loss: 0.00014542656068182883\n",
            "Train loss: 0.00014206434952072976\n",
            "Train loss: 0.0001568120378319041\n",
            "Train loss: 0.0001606965549789103\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00015970464238697398, Val f1: 1.0\n",
            "Val loss: 0.0001405838072449634, Val f1: 1.0\n",
            "Val loss: 0.0001399265913018358, Val f1: 1.0\n",
            "Val loss: 0.00014658232069320677, Val f1: 0.9999930262565613\n",
            "Val loss: 0.000142840768530732, Val f1: 0.9999943971633911\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006694450607432777, Val f1: 0.9998874664306641\n",
            "Val loss: 0.0006246167063687204, Val f1: 0.9998923540115356\n",
            "Val loss: 0.0004937813182887449, Val f1: 0.9999282360076904\n",
            "Val loss: 0.0004441808927771894, Val f1: 0.9999461770057678\n",
            "Val loss: 0.00039884546810450654, Val f1: 0.9999569654464722\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.00016986606237229092\n",
            "Train loss: 0.0001480506281950511\n",
            "Train loss: 0.00014228238343050526\n",
            "Train loss: 0.00014085840736827648\n",
            "Train loss: 0.00013823272124032624\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00014404437273956967, Val f1: 1.0\n",
            "Val loss: 0.00012825676104902023, Val f1: 1.0\n",
            "Val loss: 0.00014041609818897475, Val f1: 0.9999901056289673\n",
            "Val loss: 0.00013042633192006698, Val f1: 0.9999926090240479\n",
            "Val loss: 0.00012633993514880006, Val f1: 0.9999940395355225\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006453282888590669, Val f1: 0.9998874664306641\n",
            "Val loss: 0.000600990509459128, Val f1: 0.9998923540115356\n",
            "Val loss: 0.0004725881959462573, Val f1: 0.9999282360076904\n",
            "Val loss: 0.0004211022856503001, Val f1: 0.9999461770057678\n",
            "Val loss: 0.0003758221822661451, Val f1: 0.9999569654464722\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.0001190097669937781\n",
            "Train loss: 0.00012931862532111283\n",
            "Train loss: 0.0001399673071795232\n",
            "Train loss: 0.0001275157241512456\n",
            "Train loss: 0.00012526855115928422\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00011805098817733649, Val f1: 1.0\n",
            "Val loss: 0.00011709429839226817, Val f1: 1.0\n",
            "Val loss: 0.00012227264707603136, Val f1: 1.0\n",
            "Val loss: 0.00011666838134780327, Val f1: 1.0\n",
            "Val loss: 0.00011895721137989313, Val f1: 0.9999943971633911\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0005893870725736229, Val f1: 0.9998874664306641\n",
            "Val loss: 0.000572860638486519, Val f1: 0.9998923540115356\n",
            "Val loss: 0.0004520116696464053, Val f1: 0.9999282360076904\n",
            "Val loss: 0.00040039600844465895, Val f1: 0.9999461770057678\n",
            "Val loss: 0.0003559363355129285, Val f1: 0.9999569654464722\n",
            "\n",
            "starting Epoch 25\n",
            "Training...\n",
            "Train loss: 9.287707507610321e-05\n",
            "Train loss: 0.00011016405379840372\n",
            "Train loss: 0.00010522669520772373\n",
            "Train loss: 0.0001051378419755825\n",
            "Train loss: 0.00010838194188961228\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 8.82008476764895e-05, Val f1: 1.0\n",
            "Val loss: 9.322464829892851e-05, Val f1: 1.0\n",
            "Val loss: 8.890036364651418e-05, Val f1: 1.0\n",
            "Val loss: 9.36334561759265e-05, Val f1: 1.0\n",
            "Val loss: 9.791953096282668e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.000670836731539263, Val f1: 0.9998874664306641\n",
            "Val loss: 0.0005859905108486095, Val f1: 0.9998923540115356\n",
            "Val loss: 0.0004552538227505499, Val f1: 0.9999282360076904\n",
            "Val loss: 0.00040036948727599037, Val f1: 0.9999461770057678\n",
            "Val loss: 0.0003537319345923606, Val f1: 0.9999569654464722\n",
            "\n",
            "starting Epoch 26\n",
            "Training...\n",
            "Train loss: 0.00012179654534390595\n",
            "Train loss: 0.00010340836431298937\n",
            "Train loss: 0.00010644241544165804\n",
            "Train loss: 0.00010099616479887377\n",
            "Train loss: 9.637162619453323e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00010863426193411995, Val f1: 1.0\n",
            "Val loss: 9.530704836444265e-05, Val f1: 1.0\n",
            "Val loss: 8.963262198016136e-05, Val f1: 1.0\n",
            "Val loss: 8.701829361338502e-05, Val f1: 1.0\n",
            "Val loss: 8.604150151118769e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006846950495350433, Val f1: 0.9998874664306641\n",
            "Val loss: 0.0005828887868928076, Val f1: 0.9998923540115356\n",
            "Val loss: 0.000450094778056222, Val f1: 0.9999282360076904\n",
            "Val loss: 0.000392384794799404, Val f1: 0.9999461770057678\n",
            "Val loss: 0.0003452544918723611, Val f1: 0.9999569654464722\n",
            "\n",
            "starting Epoch 27\n",
            "Training...\n",
            "Train loss: 7.654037399333902e-05\n",
            "Train loss: 8.089242668834881e-05\n",
            "Train loss: 7.946736507056769e-05\n",
            "Train loss: 8.637570360536172e-05\n",
            "Train loss: 8.251547671014642e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 6.568596706659134e-05, Val f1: 1.0\n",
            "Val loss: 8.2672133118779e-05, Val f1: 1.0\n",
            "Val loss: 7.96416099980152e-05, Val f1: 1.0\n",
            "Val loss: 7.689923494191524e-05, Val f1: 1.0\n",
            "Val loss: 7.718450293136162e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0007007069313355411, Val f1: 0.9998874664306641\n",
            "Val loss: 0.000581963673741686, Val f1: 0.9998923540115356\n",
            "Val loss: 0.00044677276363708424, Val f1: 0.9999282360076904\n",
            "Val loss: 0.000387196842388625, Val f1: 0.9999461770057678\n",
            "Val loss: 0.000339347839083833, Val f1: 0.9999569654464722\n",
            "\n",
            "starting Epoch 28\n",
            "Training...\n",
            "Train loss: 8.11026882729493e-05\n",
            "Train loss: 7.888224619299373e-05\n",
            "Train loss: 7.297490512774814e-05\n",
            "Train loss: 7.26790835220267e-05\n",
            "Train loss: 7.545877027691209e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 8.4519758924476e-05, Val f1: 1.0\n",
            "Val loss: 8.441349018539768e-05, Val f1: 1.0\n",
            "Val loss: 8.579402904918728e-05, Val f1: 1.0\n",
            "Val loss: 8.676682205727307e-05, Val f1: 1.0\n",
            "Val loss: 8.63015996687214e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0008499717219415794, Val f1: 0.9996731281280518\n",
            "Val loss: 0.0006541642800988888, Val f1: 0.9997327923774719\n",
            "Val loss: 0.0005027668826019012, Val f1: 0.9998218417167664\n",
            "Val loss: 0.00043940143041254487, Val f1: 0.9998663663864136\n",
            "Val loss: 0.0003898903135753547, Val f1: 0.9998931288719177\n",
            "\n",
            "starting Epoch 29\n",
            "Training...\n",
            "Train loss: 8.772149314089412e-05\n",
            "Train loss: 7.935808202351577e-05\n",
            "Train loss: 7.025588619442368e-05\n",
            "Train loss: 6.753728274426457e-05\n",
            "Train loss: 6.640591679440279e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 5.8471763986744917e-05, Val f1: 1.0\n",
            "Val loss: 5.755582912700317e-05, Val f1: 1.0\n",
            "Val loss: 6.342498690216979e-05, Val f1: 1.0\n",
            "Val loss: 6.174172053111501e-05, Val f1: 1.0\n",
            "Val loss: 6.084255055092009e-05, Val f1: 1.0\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0006034149676755381, Val f1: 0.9998874664306641\n",
            "Val loss: 0.0005277398403349151, Val f1: 0.9998923540115356\n",
            "Val loss: 0.0004036165497651651, Val f1: 0.9999282360076904\n",
            "Val loss: 0.0003430299369332109, Val f1: 0.9999461770057678\n",
            "Val loss: 0.00029633129363194004, Val f1: 0.9999569654464722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс...\n",
        "\n",
        "Лучше модель справляется с твитами, в которых есть смайлики. :) :D XD и т.д... А вот без смайликов чуть сложнее. Но всё равно хорошо работает."
      ],
      "metadata": {
        "id": "h6dznCosc95q"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW2_tbkazakova_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
